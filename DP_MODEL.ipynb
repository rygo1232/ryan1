{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rygo1232/ryan1/blob/main/DP_MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKm5OTr1b4D6"
      },
      "source": [
        "# **install packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "13YuUPaKUzuB",
        "outputId": "9479b38a-3f0d-4558-8a00-27f52bf459b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.11/dist-packages (0.25.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability) (3.1.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability) (0.1.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-probability) (25.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow tensorflow-probability numpy matplotlib scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HvbwkgeHzH-7",
        "outputId": "ce09a0d1-b97c-44e4-cffa-78c5f9251786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.11/dist-packages (2.0.4)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from bayesian-optimization) (0.4.6)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from bayesian-optimization) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bayesian-optimization) (1.6.1)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bayesian-optimization) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0.0,>=1.0.0->bayesian-optimization) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0.0,>=1.0.0->bayesian-optimization) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install bayesian-optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4_48pEjh0GIQ"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade tensorflow shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Og6iiZCoM9qL"
      },
      "outputs": [],
      "source": [
        "!pip install keras_tuner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UVOv89TP8gM"
      },
      "source": [
        "# **Call Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyPqd9StL7B8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import tensorflow_probability as tfp\n",
        "from sklearn.model_selection import train_test_split,KFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score,confusion_matrix,roc_curve, auc, precision_recall_curve, precision_recall_fscore_support\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras import layers, Model, optimizers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Input, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from google.colab import drive\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.utils import shuffle\n",
        "tfd = tfp.distributions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8JvYrkMykwl"
      },
      "source": [
        "# **Data load**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkaLTFFOV7gZ"
      },
      "outputs": [],
      "source": [
        "url = 'https://github.com/rygo1232/ryan1/raw/refs/heads/main/OPR_Data_dp.xlsx'\n",
        "opr_data = pd.read_excel(url)\n",
        "opr_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFaTZO6uiRF6"
      },
      "source": [
        "# **Explaratory ANalysis**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EayD2TIZHRG"
      },
      "source": [
        "**density and histogram**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkTwEJWeXquh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot density and histogram for 'Amount_claimed'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(\n",
        "    opr_data['loss'],  # Use the specified column\n",
        "    bins=20,\n",
        "    kde=True,\n",
        "    color='MidnightBlue',\n",
        "    # The 'linewidth' parameter should be in 'line_kws' for kde plot\n",
        "    line_kws={'linewidth': 2},\n",
        "    edgecolor='black'  # Adds black edges to bars for better visibility\n",
        ")\n",
        "\n",
        "plt.title('Density Plot and Histogram of loss amount')\n",
        "plt.xlabel('loss ')\n",
        "plt.ylabel('Frequency / Density')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSY_nhNSgojw"
      },
      "source": [
        "**BOXPLOT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr2lKXEcc4dU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Remove 'loss_category' from the columns if it exists\n",
        "columns_to_plot = [col for col in opr_data.columns if col != 'loss_category']\n",
        "\n",
        "# Set a larger figure size for better visibility\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Get data for each variable\n",
        "data = [opr_data[col].dropna() for col in columns_to_plot]\n",
        "\n",
        "# Define colors for each variable\n",
        "colors = ['pink', 'blue', 'green', 'orange', 'gray', 'cyan', 'brown', 'green']\n",
        "colors = colors[:len(data)]  # Ensure the number of colors matches the data\n",
        "\n",
        "# Plot each box with specified colors\n",
        "ax = plt.gca()\n",
        "for i, (dat, color) in enumerate(zip(data, colors)):\n",
        "    bp = ax.boxplot(dat, vert=False, positions=[i+1], patch_artist=True, widths=0.7)\n",
        "    for patch in bp['boxes']:\n",
        "        patch.set_facecolor(color)\n",
        "\n",
        "# Set y-ticks to variable names\n",
        "ax.set_yticks(range(1, len(columns_to_plot) + 1))\n",
        "ax.set_yticklabels(columns_to_plot)\n",
        "\n",
        "# Improve layout and size\n",
        "plt.xlabel('Value')\n",
        "plt.title('Boxplot of operational risk Variables ')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxhjQx2ngwLy"
      },
      "source": [
        "**SKEWNESS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_R1xGajkecmi"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define the colors based on the previous description\n",
        "colors = {\n",
        "    'Execution_DP': 'gray',\n",
        "    'ICT_Failure': 'brown',\n",
        "    'External_fraud': 'blue',\n",
        "    'Employ_PWS': 'pink',\n",
        "    'Clients_PBP': 'green',\n",
        "    'Disaster': 'black',\n",
        "    'Internal_fraud': 'orange',\n",
        "    'loss': 'teal'\n",
        "}\n",
        "\n",
        "# Calculate skewness only for numerical columns\n",
        "numerical_cols = opr_data.select_dtypes(include=np.number).columns  # Select only numerical columns\n",
        "skewness = opr_data[numerical_cols].skew()\n",
        "\n",
        "# Get colors for the skewness columns, default to a general color if not specified\n",
        "bar_colors = [colors.get(col, 'grey') for col in skewness.index]\n",
        "\n",
        "# Create the barplot\n",
        "plt.figure(figsize=(10,6))\n",
        "ax = sns.barplot(x=skewness.index, y=skewness.values, palette=bar_colors)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Skewness')\n",
        "plt.title('Skewness of operational risk variables')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEddTfw-i4U7"
      },
      "source": [
        "**summary statistics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9XmKHUVDhRm"
      },
      "outputs": [],
      "source": [
        "# Generate transposed summary statistics for the opr_data DataFrame\n",
        "summary_stats = opr_data.describe().T\n",
        "print(summary_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhwcDuHEnCJa"
      },
      "outputs": [],
      "source": [
        "summary_stats = opr_data.describe().T\n",
        "skewness = opr_data.skew()\n",
        "kurtosis = opr_data.kurt()\n",
        "summary_stats['skewness'] = skewness\n",
        "summary_stats['kurtosis'] = kurtosis\n",
        "\n",
        "# Reorder columns to place skewness and kurtosis at the end\n",
        "# Get a list of original column names\n",
        "original_columns = summary_stats.columns[:-2].tolist()\n",
        "# Add 'skewness' and 'kurtosis' to the end\n",
        "all_columns = original_columns + ['skewness', 'kurtosis']\n",
        "# Reorder the columns\n",
        "summary_stats = summary_stats[all_columns]\n",
        "\n",
        "print(summary_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j7wDRDbtF35"
      },
      "source": [
        "**Load and prepare data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5Q0Gzva9x8-"
      },
      "outputs": [],
      "source": [
        "# Load and prepare data\n",
        "def load_and_prepare_data(file_path):\n",
        "    # Load data\n",
        "    opr_data = pd.read_excel(file_path)\n",
        "\n",
        "    # Create a copy of original data for reference\n",
        "    original_data = opr_data.copy()\n",
        "\n",
        "    # Exploratory Data Analysis\n",
        "    print(\"Data Overview:\")\n",
        "    print(f\"Shape: {opr_data.shape}\")\n",
        "    print(\"\\nData Types:\")\n",
        "    print(opr_data.dtypes)\n",
        "    print(\"\\nMissing Values:\")\n",
        "    print(opr_data.isnull().sum())\n",
        "\n",
        "    # Set 'Date' as index if it's not already\n",
        "    if 'Date' in opr_data.columns:\n",
        "        opr_data.set_index('Date', inplace=True)\n",
        "\n",
        "    # Handle missing values with more sophisticated approach\n",
        "    for column in opr_data.columns:\n",
        "        if opr_data[column].isnull().sum() > 0:\n",
        "            # For numeric columns, use median instead of forward fill\n",
        "            if pd.api.types.is_numeric_dtype(opr_data[column]):\n",
        "                opr_data[column].fillna(opr_data[column].median(), inplace=True)\n",
        "            else:\n",
        "                opr_data[column].fillna(opr_data[column].mode()[0], inplace=True)\n",
        "\n",
        "    # Feature Engineering: Create ratios and percentages\n",
        "    opr_data['Internal_External_Ratio'] = opr_data['Internal_fraud'] / (opr_data['External_fraud'] + 1)  # Add 1 to avoid division by zero\n",
        "    opr_data['Fraud_Total'] = opr_data['Internal_fraud'] + opr_data['External_fraud']\n",
        "    opr_data['Fraud_Percent'] = opr_data['Fraud_Total'] / (opr_data['loss'] + 1) * 100\n",
        "\n",
        "    # Create lag features for time series analysis\n",
        "    for col in ['loss', 'Internal_fraud', 'External_fraud']:\n",
        "        opr_data[f'{col}_prev_year'] = opr_data[col].shift(1)\n",
        "\n",
        "    # Drop rows with NaN values created by lag features\n",
        "    opr_data.dropna(inplace=True)\n",
        "\n",
        "    # Outlier detection using IQR method\n",
        "    numeric_cols = opr_data.select_dtypes(include=['int64', 'float64']).columns\n",
        "    for col in numeric_cols:\n",
        "        Q1 = opr_data[col].quantile(0.25)\n",
        "        Q3 = opr_data[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        # Instead of removing outliers, cap them (Winsorization)\n",
        "        opr_data[col] = np.where(opr_data[col] > upper_bound, upper_bound,\n",
        "                              np.where(opr_data[col] < lower_bound, lower_bound, opr_data[col]))\n",
        "\n",
        "    # Encode categorical features (if any)\n",
        "    categorical_cols = opr_data.select_dtypes(include=['object']).columns\n",
        "    label_encoders = {}\n",
        "    for col in categorical_cols:\n",
        "        le = LabelEncoder()\n",
        "        opr_data[col] = le.fit_transform(opr_data[col])\n",
        "        label_encoders[col] = le\n",
        "\n",
        "    # Feature selection\n",
        "    if len(opr_data.columns) > 5:  # Only if we have enough features\n",
        "        X_select = opr_data.drop('loss', axis=1)\n",
        "        y_select = opr_data['loss']\n",
        "\n",
        "        # Select top k features\n",
        "        k = min(5, len(X_select.columns))  # Select top 5 or all if less than 5\n",
        "        selector = SelectKBest(f_regression, k=k)\n",
        "        selector.fit(X_select, y_select)\n",
        "\n",
        "        # Get selected feature names\n",
        "        selected_features = X_select.columns[selector.get_support()]\n",
        "        print(\"\\nTop features selected:\")\n",
        "        print(selected_features)\n",
        "\n",
        "        # We'll keep all features but note the important ones\n",
        "\n",
        "    # Scale features - using RobustScaler for financial data which may have outliers\n",
        "    scaler = RobustScaler()  # More robust to outliers than StandardScaler\n",
        "    opr_data[numeric_cols] = scaler.fit_transform(opr_data[numeric_cols])\n",
        "\n",
        "    # Prepare features and target\n",
        "    X = opr_data.drop('loss', axis=1)\n",
        "    y = opr_data['loss']\n",
        "\n",
        "    # Shuffle and split - using time-aware split for time series\n",
        "    # Take the last 20% as test set instead of random\n",
        "    train_size = int(0.8 * len(opr_data))\n",
        "    X_train = X.iloc[:train_size]\n",
        "    X_test = X.iloc[train_size:]\n",
        "    y_train = y.iloc[:train_size]\n",
        "    y_test = y.iloc[train_size:]\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X_train = np.array(X_train)\n",
        "    X_test = np.array(X_test)\n",
        "    y_train = np.array(y_train).reshape(-1, 1)\n",
        "    y_test = np.array(y_test).reshape(-1, 1)\n",
        "\n",
        "    # Return processed data and metadata\n",
        "    return X_train, X_test, y_train, y_test, scaler, label_encoders, original_data, opr_data\n",
        "print(\"Data Preparation Complete!\")\n",
        "print(\"---------------------------\")\n",
        "print(\"Data Overview:\")\n",
        "print(f\"Shape: {opr_data.shape}\")\n",
        "print(\"\\nData Types:\")\n",
        "print(opr_data.dtypes)\n",
        "print(\"\\nMissing Values:\")\n",
        "print(opr_data.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81MlizOv5Yr6"
      },
      "source": [
        "**Feature Correlation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm6r339ika8Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Calculate the correlation matrix for numerical columns\n",
        "correlation_matrix = opr_data.corr()\n",
        "\n",
        "# To display only the relevant parts, you might select specific columns if desired\n",
        "# For example:\n",
        "# correlation_matrix = correlation_matrix[['loss', 'OtherFeature1', 'OtherFeature2']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2or5aJ4Wo-kT"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Assuming 'original_data' is your DataFrame\n",
        "\n",
        "# 1. Get feature names excluding 'loss' and 'Date'\n",
        "feature_names = [col for col in original_data.columns if col not in ['loss', 'Date']]\n",
        "\n",
        "# 2. Compute correlation with 'loss'\n",
        "correlation_with_loss = original_data[feature_names].corrwith(original_data['loss'])\n",
        "correlation_with_loss_values = correlation_with_loss.values\n",
        "\n",
        "# 3. Define your custom colors for each feature\n",
        "# Make sure the number of colors matches the number of features\n",
        "custom_colors =  ['pink', 'blue', 'green', 'orange', 'gray', 'brown', 'green']\n",
        "# Example: if you have 7 features, this list should have 7 color strings.\n",
        "\n",
        "# 4. Plot with your custom colors\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=feature_names, y=correlation_with_loss_values, palette=custom_colors)\n",
        "plt.title('Feature Correlation with Loss')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tqc3EX97-MR"
      },
      "source": [
        " **Loss Severity Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utrh882q4f6o"
      },
      "outputs": [],
      "source": [
        "loss_column = 'loss'  # Change 'Loss' to 'loss' to match the actual column name in the DataFrame\n",
        "\n",
        "# Drop any missing or non-numeric values\n",
        "loss_data = pd.to_numeric(opr_data[loss_column], errors='coerce').dropna()\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(loss_data, bins=50, color='orange', edgecolor='black')\n",
        "plt.title(\"Loss Severity Distribution\")\n",
        "plt.xlabel(\"Loss Amount\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHlxSsvz-hO_"
      },
      "source": [
        " **Contribution of Operational Risk Categories to Total Loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA0CGZ4t-ZJH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "risk_columns = [\n",
        "    'Internal_fraud', 'External_fraud', 'Employ_PWS',\n",
        "    'Clients_PBP', 'Disaster', 'ICT_Failure', 'Execution_DP'\n",
        "]\n",
        "\n",
        "# Calculate total loss per risk type\n",
        "contributions = opr_data[risk_columns].sum()\n",
        "\n",
        "# Define custom colors for each risk category\n",
        "custom_colors = [\n",
        "    '#d62728',  # Red\n",
        "    '#1f77b4',  # Blue\n",
        "    '#2ca02c',  # Green\n",
        "    '#ff7f0e',  # Orange\n",
        "    '#9467bd',  # Purple\n",
        "    '#e377c2',  # Pink\n",
        "    '#8c564b'   # Brown\n",
        "]\n",
        "\n",
        "# Plot the bar chart with custom colors\n",
        "plt.figure(figsize=(10, 6))\n",
        "contributions.plot(kind='bar', color=custom_colors, edgecolor='black')\n",
        "plt.title(\"Contribution of Operational Risk Categories to Total Loss\")\n",
        "plt.xlabel(\"Risk Category\")\n",
        "plt.ylabel(\"Total Loss Contribution (USD)\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt4SFz-VtZkx"
      },
      "source": [
        "**total loss per risk type**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3FQh1Qf8AsR"
      },
      "outputs": [],
      "source": [
        "risk_columns = [\n",
        "    'Internal_fraud', 'External_fraud', 'Employ_PWS',\n",
        "    'Clients_PBP', 'Disaster', 'ICT_Failure', 'Execution_DP'\n",
        "]\n",
        "\n",
        "# Calculate total loss per risk type\n",
        "contributions = opr_data[risk_columns].sum()\n",
        "\n",
        "# Plot the bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "contributions.plot(kind='bar', color='midnightblue', edgecolor='black')\n",
        "plt.title(\"Contribution of Operational Risk Categories to Total Loss\")\n",
        "plt.xlabel(\"Risk Category\")\n",
        "plt.ylabel(\"Total Loss Contribution (USD)\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_-6D4iNtgWi"
      },
      "source": [
        "**Loss over time, Correlation matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUEBnsM3mTe7"
      },
      "outputs": [],
      "source": [
        "# 1. Compute correlation matrix\n",
        "correlation_matrix = original_data.corr()\n",
        "\n",
        "# 2. Get feature names and correlation with 'loss'\n",
        "feature_names = [col for col in original_data.columns if col != 'loss']\n",
        "correlation_with_loss = original_data[feature_names].corrwith(original_data['loss'])\n",
        "correlation_with_loss_values = correlation_with_loss.values\n",
        "\n",
        "# 3. Plotly your visualization function as before\n",
        "def visualize_data_separately():\n",
        "    # Loss over time\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(original_data['Date'], original_data['loss'], marker='o')\n",
        "    plt.title('Loss Amount Over Time')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()\n",
        "\n",
        "    # Correlation matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "    plt.title('Correlation Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # Distribution of Loss\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.histplot(original_data['loss'], bins=20, kde=True, color='gray')\n",
        "    plt.title('Distribution of Loss')\n",
        "    plt.xlabel('Loss')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "    # Feature correlation with Loss\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.barplot(x=feature_names, y=correlation_with_loss_values)\n",
        "    plt.title('Feature Correlation with Loss')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "#Call the function\n",
        "visualize_data_separately()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxCyGNlV-AU1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Visualization function for exploratory analysis\n",
        "def visualize_data(original_data, processed_data):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "\n",
        "    # Plot loss over time\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(original_data['Date'], original_data['loss'], marker='o')\n",
        "    plt.title('Loss Over Time')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Correlation heatmap\n",
        "    plt.subplot(2, 2, 2)\n",
        "    sns.heatmap(original_data.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
        "    plt.title('Correlation Matrix')\n",
        "\n",
        "    # Distribution of loss\n",
        "    plt.subplot(2, 2, 3)\n",
        "    sns.histplot(original_data['loss'], kde=True)\n",
        "    plt.title('Distribution of Loss')\n",
        "\n",
        "    # Feature importance (using correlation with target)\n",
        "    plt.subplot(2, 2, 4)\n",
        "    corr_with_target = original_data.corr()['loss'].sort_values(ascending=False)\n",
        "    sns.barplot(x=corr_with_target.index, y=corr_with_target.values)\n",
        "    plt.title('Feature Correlation with Loss')\n",
        "    plt.xticks(rotation=90)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Load your data\n",
        "file_path = 'https://github.com/rygo1232/ryan1/raw/refs/heads/main/OPR_Data_dp.xlsx'\n",
        "X_train, X_test, y_train, y_test, scaler, label_encoders, original_data, processed_data = load_and_prepare_data(file_path)\n",
        "\n",
        "# Visualize the data\n",
        "visualize_data(original_data, processed_data)\n",
        "\n",
        "# Print the shapes to verify\n",
        "print(\"\\nShapes after conversion:\")\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DahLpG7FHUQ"
      },
      "source": [
        "# **FLowchat**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94BWll4ZFGJD"
      },
      "outputs": [],
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "dot = Digraph(comment='Operational Risk Deep Learning Pipeline')\n",
        "dot.attr(rankdir='TB', size='11,14')  # Increased size for better fit\n",
        "\n",
        "# Create clusters/subgraphs for each model type\n",
        "with dot.subgraph(name='cluster_0') as c:\n",
        "    c.attr(label='Input Processing', style='filled', color='lightgrey', fillcolor='#F5F5F5')\n",
        "    c.node('A', 'Operational Risk Data', shape='box', style='filled', fillcolor='#E6E6FA')\n",
        "    c.node('B', 'Data Preprocessing', shape='box')\n",
        "    c.node('C', 'Feature Engineering', shape='box')\n",
        "\n",
        "# ANN Model\n",
        "with dot.subgraph(name='cluster_1') as c:\n",
        "    c.attr(label='ANN Model', style='filled', color='lightgrey', fillcolor='#E6F3FF')\n",
        "    c.node('ANN1', 'Dense Layer 1', shape='box')\n",
        "    c.node('ANN2', 'Dense Layer 2', shape='box')\n",
        "    c.node('ANN3', 'Output Layer', shape='box')\n",
        "\n",
        "# LSTM Model\n",
        "with dot.subgraph(name='cluster_2') as c:\n",
        "    c.attr(label='LSTM Model', style='filled', color='lightgrey', fillcolor='#E6FFF2')\n",
        "    c.node('LSTM1', 'LSTM Layer 1', shape='box')\n",
        "    c.node('LSTM2', 'LSTM Layer 2', shape='box')\n",
        "    c.node('LSTM3', 'Output Layer', shape='box')\n",
        "\n",
        "# Autoencoder Model\n",
        "with dot.subgraph(name='cluster_3') as c:\n",
        "    c.attr(label='Autoencoder', style='filled', color='lightgrey', fillcolor='#FFF2E6')\n",
        "    c.node('AE1', 'Encoder', shape='box')\n",
        "    c.node('AE2', 'Latent Space', shape='box')\n",
        "    c.node('AE3', 'Decoder', shape='box')\n",
        "\n",
        "# Bayesian Neural Network\n",
        "with dot.subgraph(name='cluster_4') as c:\n",
        "    c.attr(label='Bayesian Neural Network', style='filled', color='lightgrey', fillcolor='#F2E6FF')\n",
        "    c.node('BNN1', 'Probabilistic Layer 1', shape='box')\n",
        "    c.node('BNN2', 'Probabilistic Layer 2', shape='box')\n",
        "    c.node('BNN3', 'Distribution Output', shape='box')\n",
        "\n",
        "# Evaluation and outputs\n",
        "with dot.subgraph(name='cluster_5') as c:\n",
        "    c.attr(label='Evaluation', style='filled', color='lightgrey', fillcolor='#FFE6E6')\n",
        "    c.node('E1', 'Predictions', shape='box')\n",
        "    c.node('E2', 'Performance Metrics', shape='box')\n",
        "    c.node('E3', 'Model Comparison', shape='box')\n",
        "    c.node('E4', 'Final Results', shape='box', style='filled', fillcolor='#FFFFD0')\n",
        "\n",
        "# Research Objectives\n",
        "dot.node('O1', 'Objective 1:\\nLatent risk factors', shape='note', style='filled', fillcolor='#FFFAE6')\n",
        "dot.node('O2', 'Objective 2:\\nHybrid models', shape='note', style='filled', fillcolor='#FFFAE6')\n",
        "dot.node('O3', 'Objective 3:\\nAnomaly detection', shape='note', style='filled', fillcolor='#FFFAE6')\n",
        "dot.node('O4', 'Objective 4:\\nUncertainty quantification', shape='note', style='filled', fillcolor='#FFFAE6')\n",
        "\n",
        "# Data flow\n",
        "dot.edge('A', 'B')\n",
        "dot.edge('B', 'C')\n",
        "\n",
        "# Connect to different models\n",
        "dot.edge('C', 'ANN1')\n",
        "dot.edge('C', 'LSTM1')\n",
        "dot.edge('C', 'AE1')\n",
        "dot.edge('C', 'BNN1')\n",
        "\n",
        "# ANN flows\n",
        "dot.edge('ANN1', 'ANN2')\n",
        "dot.edge('ANN2', 'ANN3')\n",
        "dot.edge('ANN3', 'E1')\n",
        "\n",
        "# LSTM flows\n",
        "dot.edge('LSTM1', 'LSTM2')\n",
        "dot.edge('LSTM2', 'LSTM3')\n",
        "dot.edge('LSTM3', 'E1')\n",
        "\n",
        "# Autoencoder flows\n",
        "dot.edge('AE1', 'AE2')\n",
        "dot.edge('AE2', 'AE3')\n",
        "dot.edge('AE3', 'E1')\n",
        "\n",
        "# BNN flows\n",
        "dot.edge('BNN1', 'BNN2')\n",
        "dot.edge('BNN2', 'BNN3')\n",
        "dot.edge('BNN3', 'E1')\n",
        "\n",
        "# Evaluation flows\n",
        "dot.edge('E1', 'E2')\n",
        "dot.edge('E2', 'E3')\n",
        "dot.edge('E3', 'E4')\n",
        "\n",
        "# Objectives linking to relevant models\n",
        "dot.edge('ANN3', 'O1', style='dashed')\n",
        "dot.edge('E3', 'O2', style='dashed')\n",
        "dot.edge('AE2', 'O3', style='dashed')\n",
        "dot.edge('BNN3', 'O4', style='dashed')\n",
        "\n",
        "# Render and view\n",
        "dot.render('operational_risk_models_flowchat', format='png', cleanup=True)\n",
        "dot.view()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKp8cojdf_xT"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA9nEUPpZtD2"
      },
      "source": [
        "\n",
        "# **objective 2**\n",
        "**2.\tImprove the accuracy of operational risk prediction by integrating deep learning techniques with conventional risk models.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1Nx5IKlZnjc"
      },
      "source": [
        "**CNN AND LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTqRjcBSZrL8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Conv1D, Concatenate, Dropout, GlobalMaxPooling1D, Embedding\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and prepare data\n",
        "def load_data(file_path):\n",
        "    data = opr_data\n",
        "    print(f\"Data loaded: {data.shape}\")\n",
        "    return data\n",
        "\n",
        "def preprocess_data(data):\n",
        "    # Create a copy of original data\n",
        "    original_data = data.copy()\n",
        "\n",
        "    # Set 'Date' as index if it exists\n",
        "    if 'Date' in data.columns:\n",
        "        data.set_index('Date', inplace=True)\n",
        "\n",
        "    # Handle missing values\n",
        "    for column in data.columns:\n",
        "        if data[column].isnull().sum() > 0:\n",
        "            if pd.api.types.is_numeric_dtype(data[column]):\n",
        "                data[column].fillna(data[column].median(), inplace=True)\n",
        "            else:\n",
        "                data[column].fillna(data[column].mode()[0], inplace=True)\n",
        "\n",
        "    # Feature Engineering\n",
        "    data['Internal_External_Ratio'] = data['Internal_fraud'] / (data['External_fraud'] + 1)\n",
        "    data['Fraud_Total'] = data['Internal_fraud'] + data['External_fraud']\n",
        "    data['Fraud_Percent'] = data['Fraud_Total'] / (data['loss'] + 1) * 100\n",
        "\n",
        "    # Time series features\n",
        "    for col in ['loss', 'Internal_fraud', 'External_fraud']:\n",
        "        data[f'{col}_prev_year'] = data[col].shift(1)\n",
        "\n",
        "    # Drop NaNs created by lag features\n",
        "    data.dropna(inplace=True)\n",
        "\n",
        "    # Handle outliers with Winsorization\n",
        "    numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
        "    for col in numeric_cols:\n",
        "        Q1, Q3 = data[col].quantile(0.25), data[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
        "        data[col] = np.clip(data[col], lower, upper)\n",
        "\n",
        "    # Encode categorical features\n",
        "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
        "    encoders = {}\n",
        "    for col in categorical_cols:\n",
        "        le = LabelEncoder()\n",
        "        data[col] = le.fit_transform(data[col])\n",
        "        encoders[col] = le\n",
        "\n",
        "    # Scale numeric features\n",
        "    scaler = RobustScaler()\n",
        "    data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
        "\n",
        "    # Extract features and target\n",
        "    X = data.drop('loss', axis=1)\n",
        "    y = data['loss']\n",
        "\n",
        "    # Time-aware split (80% train, 20% test)\n",
        "    train_size = int(0.8 * len(data))\n",
        "    X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
        "    y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
        "\n",
        "    # Feature importance\n",
        "    if len(X.columns) > 5:\n",
        "        selector = SelectKBest(f_regression, k=min(5, len(X.columns)))\n",
        "        selector.fit(X_train, y_train)\n",
        "        top_features = X.columns[selector.get_support()]\n",
        "        print(\"Top features:\", top_features.tolist())\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, scaler, encoders, original_data, data\n",
        "\n",
        "# Build the hybrid CNN-LSTM model\n",
        "def build_model(X_num, X_text=None):\n",
        "    # Input layers\n",
        "    num_input = Input(shape=(X_num.shape[1],), name='numerical_input')\n",
        "\n",
        "    # Reshape for LSTM using Keras's Reshape layer\n",
        "    reshaped = tf.keras.layers.Reshape((1, X_num.shape[1]))(num_input) # Reshape using Keras layer\n",
        "\n",
        "    # LSTM for numerical data (Equation 21)\n",
        "    lstm_out = LSTM(64, return_sequences=False)(reshaped)\n",
        "\n",
        "    # If text data is available\n",
        "    if X_text is not None:\n",
        "        text_input = Input(shape=(X_text.shape[1],), name='text_input')\n",
        "\n",
        "        # Embedding layer for text\n",
        "        embedding = Embedding(input_dim=10000, output_dim=100)(text_input)\n",
        "\n",
        "        # CNN for textual data (Equation 20)\n",
        "        conv = Conv1D(filters=64, kernel_size=3, activation='relu')(embedding)\n",
        "        pooled = GlobalMaxPooling1D()(conv)\n",
        "\n",
        "        # Feature combination (Equation 22)\n",
        "        combined = Concatenate()([lstm_out, pooled])\n",
        "    else:\n",
        "        combined = lstm_out\n",
        "\n",
        "    # Fully connected layers\n",
        "    dense1 = Dense(64, activation='relu')(combined)\n",
        "    dropout = Dropout(0.3)(dense1)\n",
        "    dense2 = Dense(32, activation='relu')(dropout)\n",
        "\n",
        "    # Output layer (Equation 23)\n",
        "    output = Dense(1, name='output')(dense2)\n",
        "\n",
        "    # Define model inputs based on available data\n",
        "    if X_text is not None:\n",
        "        model = Model(inputs=[num_input, text_input], outputs=output)\n",
        "    else:\n",
        "        model = Model(inputs=num_input, outputs=output)\n",
        "\n",
        "    # Custom loss function approximating Equation 24\n",
        "    # Using MSE with L1 and L2 regularization\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss=tf.keras.losses.MeanSquaredError(),\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Train and evaluate model with improved training process\n",
        "def train_evaluate_model(X_train, X_test, y_train, y_test):\n",
        "    # Check if we have text data\n",
        "    has_text = False  # Set to True if textual data is available\n",
        "\n",
        "    # Create a validation set from training data\n",
        "    X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
        "        X_train, y_train, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # For now, we'll assume only numerical data\n",
        "    model = build_model(X_train)\n",
        "\n",
        "    # Early stopping with reduced patience\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=15,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Learning rate reduction on plateau\n",
        "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=0.00001,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Train model with explicit validation data\n",
        "    history = model.fit(\n",
        "        X_train_final, y_train_final,\n",
        "        epochs=200,  # Increased epochs with early stopping\n",
        "        batch_size=16,  # Smaller batch size for better generalization\n",
        "        validation_data=(X_val, y_val),  # Explicit validation data\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate model\n",
        "    eval_results = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"Test loss (MSE): {eval_results[0]:.4f}\")\n",
        "    print(f\"Test MAE: {eval_results[1]:.4f}\")\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate R score\n",
        "    from sklearn.metrics import r2_score\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"R score: {r2:.4f}\")\n",
        "\n",
        "    return model, history, y_pred\n",
        "\n",
        "# Enhanced visualization of results\n",
        "def visualize_results(history, y_test, y_pred):\n",
        "    # Create a figure with 2x2 subplots for more comprehensive analysis\n",
        "    plt.figure(figsize=(16, 14))\n",
        "\n",
        "    # 1. Training and validation loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss', fontsize=14)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss (MSE)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Predictions vs Actual (scatter plot)\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.scatter(y_test, y_pred, alpha=0.7)\n",
        "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')\n",
        "    plt.title('Predictions vs Actual', fontsize=14)\n",
        "    plt.xlabel('Actual')\n",
        "    plt.ylabel('Predicted')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Residuals plot\n",
        "    plt.subplot(2, 2, 3)\n",
        "    residuals = y_test - y_pred.flatten()\n",
        "    plt.scatter(y_pred.flatten(), residuals, alpha=0.7)\n",
        "    plt.axhline(y=0, color='r', linestyle='--')\n",
        "    plt.title('Residuals Plot', fontsize=14)\n",
        "    plt.xlabel('Predicted Values')\n",
        "    plt.ylabel('Residuals')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Training and validation MAE\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(history.history['mae'], label='Train MAE')\n",
        "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "    plt.title('Model MAE', fontsize=14)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Mean Absolute Error')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Feature importance analysis (if possible)\n",
        "    try:\n",
        "        # Create a synthetic feature importance visualization\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        # We'll use the weights of the first dense layer as a proxy for feature importance\n",
        "        weights = model.layers[1].get_weights()[0]\n",
        "        importance = np.mean(np.abs(weights), axis=1)\n",
        "        feature_names = [f\"Feature {i+1}\" for i in range(len(importance))]\n",
        "\n",
        "        # Sort by importance\n",
        "        sorted_idx = np.argsort(importance)\n",
        "        plt.barh(range(len(sorted_idx)), importance[sorted_idx])\n",
        "        plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
        "        plt.title('Feature Importance (Approximation)', fontsize=14)\n",
        "        plt.xlabel('Mean Absolute Weight')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    except:\n",
        "        print(\"Feature importance visualization not available\")\n",
        "\n",
        "# Main function\n",
        "def main(file_path):\n",
        "    # Load and preprocess data\n",
        "    data = load_data(opr_data)\n",
        "    X_train, X_test, y_train, y_test, scaler, encoders, original_data, processed_data = preprocess_data(data)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X_train = np.array(X_train)\n",
        "    X_test = np.array(X_test)\n",
        "    y_train = np.array(y_train)\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "    # Train and evaluate model\n",
        "    model, history, y_pred = train_evaluate_model(X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # Visualize results\n",
        "    visualize_results(history, y_test, y_pred)\n",
        "\n",
        "    return model, scaler, encoders, original_data, processed_data\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with actual file path\n",
        "    model, scaler, encoders, original_data, processed_data = main(opr_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64t32ph4dWfW"
      },
      "source": [
        "\n",
        "\n",
        " **Prediction accuracy**\n",
        "\n",
        "The effectiveness of the deep learning-enhanced risk prediction model was evaluated by comparing its predicted values against actual figures across multiple operational risk metrics. Figure above illustrates the correlation between predicted and observed values for various key indicators. The integration of deep learning techniques aims to enhance the accuracy of operational risk assessments, facilitating more reliable identification and management of potential operational failures.\n",
        "\n",
        "Figures  and Table  demonstrate the high accuracy of the deep learning-enhanced operational risk prediction model. The close alignment between the actual and predicted values across key metrics indicates the effectiveness of integrating neural network techniques with traditional risk models. The near-perfect correlation along the ideal line (red) reflects the models ability to precisely estimate operational risk indicators, thereby supporting the objective of enhancing risk assessment accuracy through advanced machine learning methods.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LzJ3lY9hiXsN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Input\n",
        "from keras.optimizers import Adam\n",
        "from keras import regularizers\n",
        "\n",
        "# Load data\n",
        "def load_data(file_path=None):\n",
        "    data = pd.DataFrame({\n",
        "        'Date': list(range(1980, 2025)),\n",
        "        'loss': [2955, 33260, 52784, 4072, 24008, 1458, 3145, 2365, 3368, 16915, 37217, 38318, 39303, 47121, 41121, 29164, 32359, 25569, 26214, 11862, 32463, 39833, 23582, 29454, 39713, 42097, 26561, 21927, 40351, 26454, 39388, 38436, 24708, 28622, 24711, 30471, 26391, 46784, 40167, 33505, 27552, 33032, 24779, 34706, 31638],\n",
        "        'Internal_fraud': [99, 891, 950, 161, 158, 11, 2, 13, 312, 162, 895, 997, 537, 560, 817, 46, 461, 571, 602, 416, 842, 297, 940, 189, 169, 928, 282, 730, 146, 162, 324, 215, 160, 653, 348, 877, 868, 287, 430, 478, 445, 515, 435, 204, 721],\n",
        "        'External_fraud': [465, 589, 7045, 304, 4514, 104, 104, 36, 35, 105, 5207, 75, 8140, 3759, 1948, 4280, 8799, 4, 4724, 332, 1659, 9442, 534, 9323, 3729, 3242, 2653, 3078, 7668, 5383, 5249, 3215, 7443, 6752, 675, 6161, 3866, 1310, 5960, 185, 1476, 4223, 2296, 4337, 7602],\n",
        "        'EmployPWS': [77, 291, 3395, 234, 240, 27, 69, 58, 677, 1459, 440, 4027, 2143, 3887, 1544, 652, 4116, 4424, 2957, 4841, 991, 1577, 4124, 841, 664, 3860, 1759, 197, 1478, 4468, 4507, 2357, 1425, 4416, 3474, 509, 4054, 4129, 1349, 4900, 2994, 1621, 261, 4414, 3235],\n",
        "        'ClientsPBP': [1816, 18275, 26504, 2456, 12962, 76, 943, 1460, 1825, 9001, 16878, 20482, 12306, 26023, 26863, 13346, 13931, 14744, 15153, 2499, 20738, 21300, 13309, 2633, 22829, 26863, 17435, 2384, 22413, 6751, 17139, 22276, 8373, 3649, 17188, 13977, 6505, 26620, 18478, 18571, 8517, 21890, 16469, 13170, 6324],\n",
        "        'Disaster': [40, 9, 390, 2, 30, 4, 3, 0, 3, 656, 319, 402, 114, 679, 477, 185, 132, 613, 529, 78, 513, 55, 127, 638, 437, 187, 206, 149, 267, 391, 14, 392, 169, 477, 138, 214, 49, 574, 100, 96, 65, 27, 605, 410, 192],\n",
        "        'ICTFailure': [50, 3851, 844, 28, 678, 507, 53, 33, 76, 468, 1267, 3318, 1445, 604, 246, 291, 1061, 1894, 142, 2124, 3211, 2374, 1894, 3824, 908, 1296, 3180, 509, 343, 2256, 2137, 3823, 3745, 2028, 1942, 3037, 1111, 2923, 2585, 2554, 2928, 3749, 3112, 2699, 3926],\n",
        "        'ExecutionDP': [408, 9353, 13656, 887, 5425, 729, 1971, 765, 441, 5065, 12210, 9016, 14618, 11609, 9226, 10364, 3858, 3319, 2107, 1572, 4510, 4789, 2656, 12005, 10978, 5722, 1047, 14879, 8035, 7042, 10019, 6157, 3393, 10648, 946, 5697, 9939, 10940, 11265, 6722, 11128, 1007, 1601, 9472, 9638]\n",
        "    })\n",
        "    print(f\"Data loaded: {data.shape}\")\n",
        "    return data\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_data()\n",
        "\n",
        "# Data Preprocessing\n",
        "features = ['Internal_fraud', 'External_fraud', 'EmployPWS', 'ClientsPBP', 'Disaster', 'ICTFailure', 'ExecutionDP']\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(dataset[features])\n",
        "\n",
        "# PCA for visualization\n",
        "pca = PCA(n_components=2)\n",
        "principal_components = pca.fit_transform(X_scaled)\n",
        "dataset['PCA1'] = principal_components[:, 0]\n",
        "dataset['PCA2'] = principal_components[:, 1]\n",
        "\n",
        "# KMeans Clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "dataset['Cluster'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Visualization: Clustering\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='PCA1', y='PCA2', hue='Cluster', data=dataset, palette='viridis')\n",
        "plt.title('Clustering of Operational Risk Factors')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n",
        "\n",
        "# Feature Importance using RandomForestRegressor\n",
        "X = dataset[features]\n",
        "y = dataset['loss']\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X, y)\n",
        "importance = model.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': importance})\n",
        "\n",
        "# Visualization: Feature Importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importance_df.sort_values(by='Importance', ascending=False))\n",
        "plt.title('Feature Importance Rankings (Operational Risk)')\n",
        "plt.show()\n",
        "\n",
        "# Train a CNN model to get actual training/validation loss\n",
        "# Reshape features for CNN\n",
        "X_scaled_reshaped = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled_reshaped, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a simple CNN for regression\n",
        "inputs = Input(shape=(X_scaled.shape[1], 1))\n",
        "conv1 = Conv1D(filters=16, kernel_size=2, activation='relu', kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
        "pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
        "flatten = Flatten()(pool1)\n",
        "dense1 = Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.01))(flatten)\n",
        "outputs = Dense(1)(dense1)\n",
        "cnn_model = Model(inputs=inputs, outputs=outputs)\n",
        "cnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "# Train CNN and capture history\n",
        "history = cnn_model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test), verbose=1)\n",
        "\n",
        "# Loss plot using actual CNN training history\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='green')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss (CNN for Operational Risk)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Predict loss using RandomForestRegressor\n",
        "model.fit(X, y)\n",
        "predicted = model.predict(X)\n",
        "\n",
        "# Actual vs Predicted scatter plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y, predicted, color='blue', label='Predictions')\n",
        "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', label='Ideal')\n",
        "plt.xlabel('Actual Loss')\n",
        "plt.ylabel('Predicted Loss')\n",
        "plt.title('Actual vs Predicted Loss (RandomForestRegressor)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Detailed Result Table\n",
        "results_df = pd.DataFrame({\n",
        "    'Year': dataset['Date'],\n",
        "    'Actual Loss': y,\n",
        "    'Predicted Loss': predicted,\n",
        "    'Absolute Error': np.abs(y - predicted)\n",
        "})\n",
        "\n",
        "print(\"Detailed Results:\")\n",
        "print(results_df)\n",
        "\n",
        "# Calculate Mean Absolute Error (MAE)\n",
        "mae = np.mean(np.abs(y - predicted))\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "\n",
        "# Risk analysis: Risk score distribution and reduction percentage (synthetic)\n",
        "risk_factors = ['Internal Fraud', 'External Fraud', 'Employment Issues', 'Client Issues', 'Disaster', 'ICT Failure', 'Execution Issues']\n",
        "risk_before = np.random.uniform(0.6, 0.8, len(risk_factors))\n",
        "risk_after = risk_before - np.random.uniform(0.2, 0.4, len(risk_factors))\n",
        "risk_reduction = (risk_before - risk_after) / risk_before * 100\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(risk_factors, risk_reduction, color='teal')\n",
        "plt.title('Risk Reduction Percentage by Risk Type')\n",
        "plt.xlabel('Risk Type')\n",
        "plt.ylabel('Risk Reduction Percentage')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdUM_xDMluIY"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGWayAGFd-I_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Input\n",
        "from keras.optimizers import Adam\n",
        "from keras import regularizers\n",
        "\n",
        "# Load data\n",
        "def load_data(file_path=None):\n",
        "    data = pd.DataFrame({\n",
        "        'Date': list(range(1980, 2025)),\n",
        "        'loss': [2955, 33260, 52784, 4072, 24008, 1458, 3145, 2365, 3368, 16915, 37217, 38318, 39303, 47121, 41121, 29164, 32359, 25569, 26214, 11862, 32463, 39833, 23582, 29454, 39713, 42097, 26561, 21927, 40351, 26454, 39388, 38436, 24708, 28622, 24711, 30471, 26391, 46784, 40167, 33505, 27552, 33032, 24779, 34706, 31638],\n",
        "        'Internal_fraud': [99, 891, 950, 161, 158, 11, 2, 13, 312, 162, 895, 997, 537, 560, 817, 46, 461, 571, 602, 416, 842, 297, 940, 189, 169, 928, 282, 730, 146, 162, 324, 215, 160, 653, 348, 877, 868, 287, 430, 478, 445, 515, 435, 204, 721],\n",
        "        'External_fraud': [465, 589, 7045, 304, 4514, 104, 104, 36, 35, 105, 5207, 75, 8140, 3759, 1948, 4280, 8799, 4, 4724, 332, 1659, 9442, 534, 9323, 3729, 3242, 2653, 3078, 7668, 5383, 5249, 3215, 7443, 6752, 675, 6161, 3866, 1310, 5960, 185, 1476, 4223, 2296, 4337, 7602],\n",
        "        'EmployPWS': [77, 291, 3395, 234, 240, 27, 69, 58, 677, 1459, 440, 4027, 2143, 3887, 1544, 652, 4116, 4424, 2957, 4841, 991, 1577, 4124, 841, 664, 3860, 1759, 197, 1478, 4468, 4507, 2357, 1425, 4416, 3474, 509, 4054, 4129, 1349, 4900, 2994, 1621, 261, 4414, 3235],\n",
        "        'ClientsPBP': [1816, 18275, 26504, 2456, 12962, 76, 943, 1460, 1825, 9001, 16878, 20482, 12306, 26023, 26863, 13346, 13931, 14744, 15153, 2499, 20738, 21300, 13309, 2633, 22829, 26863, 17435, 2384, 22413, 6751, 17139, 22276, 8373, 3649, 17188, 13977, 6505, 26620, 18478, 18571, 8517, 21890, 16469, 13170, 6324],\n",
        "        'Disaster': [40, 9, 390, 2, 30, 4, 3, 0, 3, 656, 319, 402, 114, 679, 477, 185, 132, 613, 529, 78, 513, 55, 127, 638, 437, 187, 206, 149, 267, 391, 14, 392, 169, 477, 138, 214, 49, 574, 100, 96, 65, 27, 605, 410, 192],\n",
        "        'ICTFailure': [50, 3851, 844, 28, 678, 507, 53, 33, 76, 468, 1267, 3318, 1445, 604, 246, 291, 1061, 1894, 142, 2124, 3211, 2374, 1894, 3824, 908, 1296, 3180, 509, 343, 2256, 2137, 3823, 3745, 2028, 1942, 3037, 1111, 2923, 2585, 2554, 2928, 3749, 3112, 2699, 3926],\n",
        "        'ExecutionDP': [408, 9353, 13656, 887, 5425, 729, 1971, 765, 441, 5065, 12210, 9016, 14618, 11609, 9226, 10364, 3858, 3319, 2107, 1572, 4510, 4789, 2656, 12005, 10978, 5722, 1047, 14879, 8035, 7042, 10019, 6157, 3393, 10648, 946, 5697, 9939, 10940, 11265, 6722, 11128, 1007, 1601, 9472, 9638]\n",
        "    })\n",
        "    print(f\"Data loaded: {data.shape}\")\n",
        "    return data\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_data()\n",
        "\n",
        "# Data Preprocessing\n",
        "features = ['Internal_fraud', 'External_fraud', 'EmployPWS', 'ClientsPBP', 'Disaster', 'ICTFailure', 'ExecutionDP']\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(dataset[features])\n",
        "\n",
        "# PCA for visualization\n",
        "pca = PCA(n_components=2)\n",
        "principal_components = pca.fit_transform(X_scaled)\n",
        "dataset['PCA1'] = principal_components[:, 0]\n",
        "dataset['PCA2'] = principal_components[:, 1]\n",
        "\n",
        "# KMeans Clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "dataset['Cluster'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Visualization: Clustering with stars (x markers), custom colors, and larger font\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='PCA1', y='PCA2', hue='Cluster', data=dataset, palette=['red', 'green', 'blue'], marker='x')\n",
        "plt.title('Clustering of Operational Risk Factors', fontsize=14)\n",
        "plt.xlabel('Principal Component 1', fontsize=14)\n",
        "plt.ylabel('Principal Component 2', fontsize=14)\n",
        "plt.legend(title='Cluster', fontsize=14, title_fontsize=14)\n",
        "plt.savefig('clustering.png')\n",
        "\n",
        "# Create the Clustering Table (Classify variables by clusters)\n",
        "# Calculate the mean of each feature for each cluster\n",
        "cluster_means = dataset.groupby('Cluster')[features].mean()\n",
        "\n",
        "# Classify variables into clusters based on highest mean\n",
        "cluster_0_vars = []\n",
        "cluster_1_vars = []\n",
        "cluster_2_vars = []\n",
        "\n",
        "for feature in features:\n",
        "    means = cluster_means[feature]\n",
        "    max_cluster = means.idxmax()\n",
        "    if max_cluster == 0:\n",
        "        cluster_0_vars.append(feature)\n",
        "    elif max_cluster == 1:\n",
        "        cluster_1_vars.append(feature)\n",
        "    elif max_cluster == 2:\n",
        "        cluster_2_vars.append(feature)\n",
        "\n",
        "# Ensure all lists are the same length for table creation by padding with empty strings\n",
        "max_length = max(len(cluster_0_vars), len(cluster_1_vars), len(cluster_2_vars))\n",
        "cluster_0_vars.extend([''] * (max_length - len(cluster_0_vars)))\n",
        "cluster_1_vars.extend([''] * (max_length - len(cluster_1_vars)))\n",
        "cluster_2_vars.extend([''] * (max_length - len(cluster_2_vars)))\n",
        "\n",
        "# Prepare the table data\n",
        "cluster_table_data = pd.DataFrame({\n",
        "    'Cluster 0': cluster_0_vars,\n",
        "    'Cluster 1': cluster_1_vars,\n",
        "    'Cluster 2': cluster_2_vars\n",
        "})\n",
        "\n",
        "# Plot the clustering table\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "table = ax.table(cellText=cluster_table_data.values, colLabels=cluster_table_data.columns, cellLoc='center', loc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1.2, 1.2)\n",
        "\n",
        "plt.savefig('cluster_table.png')\n",
        "\n",
        "# Feature Importance using RandomForestRegressor for 'loss'\n",
        "X = dataset[features]\n",
        "y_loss = dataset['loss']\n",
        "model_loss = RandomForestRegressor(random_state=42)\n",
        "model_loss.fit(X, y_loss)\n",
        "importance = model_loss.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': importance})\n",
        "\n",
        "# Visualization: Feature Importance for 'loss'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importance_df.sort_values(by='Importance', ascending=False))\n",
        "plt.title('Feature Importance Rankings for Loss (Operational Risk)')\n",
        "plt.savefig('feature_importance.png')\n",
        "\n",
        "# Train a CNN model for 'loss' to get actual training/validation loss\n",
        "X_scaled_reshaped = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))\n",
        "X_train, X_test, y_train_loss, y_test_loss = train_test_split(X_scaled_reshaped, y_loss, test_size=0.2, random_state=42)\n",
        "\n",
        "inputs = Input(shape=(X_scaled.shape[1], 1))\n",
        "conv1 = Conv1D(filters=16, kernel_size=2, activation='relu', kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
        "pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
        "flatten = Flatten()(pool1)\n",
        "dense1 = Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.01))(flatten)\n",
        "outputs = Dense(1)(dense1)\n",
        "cnn_model = Model(inputs=inputs, outputs=outputs)\n",
        "cnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "history = cnn_model.fit(X_train, y_train_loss, epochs=50, batch_size=16, validation_data=(X_test, y_test_loss), verbose=1)\n",
        "\n",
        "# Loss plot using actual CNN training history\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='green')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss (CNN for Loss Prediction)')\n",
        "plt.legend()\n",
        "plt.savefig('cnn_loss.png')\n",
        "\n",
        "# Train separate RandomForestRegressor for each feature\n",
        "models = {}\n",
        "predictions = {}\n",
        "for feature in features:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, dataset[feature], test_size=0.2, random_state=42)\n",
        "    model = RandomForestRegressor(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    models[feature] = model\n",
        "    predictions[feature] = model.predict(X_scaled)\n",
        "\n",
        "# Actual vs Predicted plots for each feature\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, feature in enumerate(features):\n",
        "    plt.subplot(3, 3, i+1)\n",
        "    plt.scatter(dataset[feature], predictions[feature], color='blue', label='Predictions')\n",
        "    plt.plot([dataset[feature].min(), dataset[feature].max()], [dataset[feature].min(), dataset[feature].max()], 'r--', label='Ideal')\n",
        "    plt.xlabel(f'Actual {feature}')\n",
        "    plt.ylabel(f'Predicted {feature}')\n",
        "    plt.title(f'Actual vs Predicted {feature} (Operational Risk)')\n",
        "    plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('actual_vs_predicted.png')\n",
        "\n",
        "# Detailed Results Table for all features\n",
        "results_df = pd.DataFrame({\n",
        "    'Year': dataset['Date'],\n",
        "    **{f'Actual {feature}': dataset[feature] for feature in features},\n",
        "    **{f'Predicted {feature}': predictions[feature] for feature in features},\n",
        "    **{f'Absolute Error {feature}': np.abs(dataset[feature] - predictions[feature]) for feature in features}\n",
        "})\n",
        "\n",
        "print(\"Detailed Results:\")\n",
        "#print(results_df)\n",
        "\n",
        "# Calculate Mean Absolute Error (MAE) for each feature\n",
        "mae_dict = {feature: np.mean(np.abs(dataset[feature] - predictions[feature])) for feature in features}\n",
        "mae_df = pd.DataFrame(list(mae_dict.items()), columns=['Feature', 'MAE'])\n",
        "print(\"\\nMean Absolute Error (MAE) per Feature:\")\n",
        "print(mae_df)\n",
        "\n",
        "# Create the Actual vs Predicted Table\n",
        "table_data = pd.DataFrame({\n",
        "    'variable': ['Internal_fraud', 'External_fraud', 'EmployPWS', 'ClientsPBP', 'Disaster', 'ICTFailure', 'ExecutionDP'],\n",
        "    'Actual value': [\n",
        "        dataset['Internal_fraud'].mean(),\n",
        "        dataset['External_fraud'].mean(),\n",
        "        dataset['EmployPWS'].mean(),\n",
        "        dataset['ClientsPBP'].mean(),\n",
        "        dataset['Disaster'].mean(),\n",
        "        dataset['ICTFailure'].mean(),\n",
        "        dataset['ExecutionDP'].mean()\n",
        "    ],\n",
        "    'Predicted value': [\n",
        "        predictions['Internal_fraud'].mean(),\n",
        "        predictions['External_fraud'].mean(),\n",
        "        predictions['EmployPWS'].mean(),\n",
        "        predictions['ClientsPBP'].mean(),\n",
        "        predictions['Disaster'].mean(),\n",
        "        predictions['ICTFailure'].mean(),\n",
        "        predictions['ExecutionDP'].mean()\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Round the numerical columns to remove decimal places\n",
        "table_data['Actual value'] = table_data['Actual value'].round(0).astype(int)\n",
        "table_data['Predicted value'] = table_data['Predicted value'].round(0).astype(int)\n",
        "\n",
        "# Plot the table\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "table = ax.table(cellText=table_data.values, colLabels=table_data.columns, cellLoc='center', loc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1.2, 1.2)\n",
        "\n",
        "plt.savefig('actual_vs_predicted_table.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMT0o9zIV92m"
      },
      "source": [
        "# **Obejective 1**\n",
        "**Develop deep learning models to uncover latent operational risk factors that traditional methods fail to detect.**\n",
        "\n",
        "The deep learning modelsBayesian Neural Networks (BNN), Convolutional Neural Networks (CNN), Recurrent Neural Networks with LSTM (RNN), and Autoencoders with GAN-like principleshave been developed to identify latent operational risk factors that traditional methods, such as simple statistical thresholds, often miss due to their inability to capture non-linear and temporal dependencies. These models were trained on a dataset spanning 19802025, with preprocessing steps like normalization and reshaping applied to leverage its time-series nature. An ensemble approach, combining predictions from all models, was implemented to enhance detection robustness. Synthetic anomalies (5% rate) were injected into the test set (derived from a 45-sample dataset) to evaluate performance, revealing the models' potential to uncover hidden risks, though limitations in dataset size and anomaly rate impacted results.\n",
        "\n",
        "**Ensemble Anomaly Detection**\n",
        "\n",
        "The ensemble anomaly detection plot demonstrates the model's capability to identify latent risks by showing reconstruction errors across test samples, with a threshold of approximately 15 (95th percentile) marking anomalies. One sample (index 7) exceeds this threshold with an error of about 25, indicating a significant latent risk factor. The corresponding test data plot highlights this anomaly in red (normalized value ~2.0), while other samples (green, -1.0 to 1.0) are classified as normal. This showcases the ensemble's strength in capturing complex patterns, surpassing traditional methods. However, the low precision, recall, and F1-score (0.11) suggest under-detection, likely due to the small dataset and low anomaly rate. Adjusting the threshold or increasing the anomaly rate could improve sensitivity.\n",
        "\n",
        "(Insert Image: Ensemble Anomaly Detection)\n",
        "\n",
        "**Performance Metrics**\n",
        "\n",
        "Performance metrics across all models (BNN, CNN, RNN, Autoencoder, Ensemble) show a consistent precision, recall, and F1-score of 0.11, indicating limited ability to distinguish anomalies from normal data. Training times (e.g., 95 ms/step for BNN, 85 ms/step for CNN) confirm efficient execution, but the low F1-score highlights the need for tuning. The small dataset (45 samples) and 5% synthetic anomaly rate likely restrict robust learning, suggesting that expanding the dataset or adjusting the anomaly injection could enhance detection accuracy.\n",
        "\n",
        "(Insert Image: Performance Metrics)\n",
        "\n",
        "**Confusion Matrix**\n",
        "\n",
        "The confusion matrix heatmap reveals the ensemble model's performance, with 8 true negatives and 1 true positive, excelling at classifying normal samples but detecting only one anomaly. The absence of false positives and negatives aligns with the low 0.11 F1-score, indicating significant under-detection due to the conservative 95th percentile threshold and limited dataset (45 samples) with a 5% anomaly rate. This suggests the model misses many latent risks, an area where traditional methods also falter. Lowering the threshold or increasing the anomaly rate could boost sensitivity.\n",
        "\n",
        "(Insert Image: Confusion Matrix for Ensemble Model)\n",
        "\n",
        "**Feature Importance in Latent Space**\n",
        "\n",
        "The feature importance plot, derived from the Autoencoder, highlights key operational risk factors influencing the latent space. EmployPWS, ClientsPBP, External_fraud, Internal_fraud, ICT_Failure, and Disaster score between 0.15 and 0.16, indicating their dominance as risk drivers, while ExecutionDP scores lower (~0.12), possibly due to stability. This prioritization, which traditional methods like simple averages miss, reveals non-linear relationships, though the narrow score range (0.120.16) suggests the Autoencoders limited ability to differentiate critical factors, potentially due to dataset size. Enhancing data or model complexity could refine these insights.\n",
        "\n",
        "(Insert Image: Feature Importance in Latent Space)\n",
        "\n",
        "**Latent Space Representation**\n",
        "\n",
        "The latent space representation from the Autoencoder visualizes the reduced-dimensional structure of the risk data across two dimensions (0.00.7), reflecting underlying variability without distinct clusters. This scattered distribution indicates some learned structure, surpassing traditional methods like linear regression or basic PCA, which struggle with non-linear dependencies. However, the lack of clustering, likely due to the small dataset (45 samples) or simple architecture, limits clear risk profile separation. Increasing data or adding layers could enhance cluster identification for targeted mitigation.\n",
        "\n",
        "(Insert Image: Latent Space Representation)\n",
        "\n",
        "**Time-Series Anomaly Trend (Internal Fraud)**\n",
        "\n",
        "The time-series anomaly trend for Internal_fraud tracks its normalized values (-2.0 to 3.0) over time, with two anomalies detected at indices 0 (~-0.5) and 5 (~2.0), marked in red. This leverages LSTMs temporal modeling to uncover risks traditional methods, like static thresholds, miss. The low detection rate, consistent with the 0.11 F1-score, reflects the small dataset (45 samples) and 5% anomaly rate, suggesting under-detection. Increasing the anomaly rate or dataset size could reveal more latent risk spikes for proactive management.\n",
        "\n",
        "(Insert Image: Time-Series Anomaly Trend (Internal Fraud))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QiNRMi0x0vJR"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Preprocess data\n",
        "data = opr_data\n",
        "features = ['Internal_fraud', 'External_fraud', 'Employ_PWS', 'Clients_PBP', 'Disaster', 'ICT_Failure', 'Execution_DP']\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(data[features])\n",
        "\n",
        "# Reshape for CNN and RNN (time series-like)\n",
        "X_reshaped = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))\n",
        "X_train, X_test = train_test_split(X_reshaped, test_size=0.2, random_state=42)\n",
        "\n",
        "# Synthetic anomalies for testing\n",
        "def add_synthetic_anomalies(data, anomaly_rate=0.05):\n",
        "    # Ensure anomaly_rate results in at least one anomaly\n",
        "    num_anomalies = int(data.shape[0] * anomaly_rate)\n",
        "    if num_anomalies == 0:\n",
        "        num_anomalies = 1  # Minimum 1 anomaly\n",
        "\n",
        "    anomalies = np.zeros(data.shape[0], dtype=int)\n",
        "    anomalies[:num_anomalies] = 1  # Mark first num_anomalies as anomalies\n",
        "    np.random.shuffle(anomalies)  # Randomize\n",
        "\n",
        "    anomaly_mask = anomalies == 1\n",
        "    data_with_anomalies = data.copy()\n",
        "    # Reshape noise to match data shape\n",
        "    noise_shape = (anomaly_mask.sum(), data.shape[1], data.shape[2])\n",
        "    data_with_anomalies[anomaly_mask] += np.random.normal(0, 5, size=noise_shape)\n",
        "    return data_with_anomalies, anomaly_mask\n",
        "\n",
        "X_test_with_anomalies, test_anomalies = add_synthetic_anomalies(X_test)\n",
        "# a) Bayesian Neural Network (BNN)\n",
        "def create_bnn_model(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Dense(32, activation='relu')(inputs)\n",
        "    x = layers.Dense(16, activation='relu')(x)\n",
        "    outputs = layers.Dense(input_shape[1], activation='linear')(x)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "bnn_model = create_bnn_model((X_reshaped.shape[1], 1))\n",
        "bnn_model.compile(optimizer='adam', loss='mse')\n",
        "bnn_model.fit(X_train, X_train, epochs=20, batch_size=16, validation_split=0.2, verbose=0)\n",
        "bnn_pred = bnn_model.predict(X_test_with_anomalies)\n",
        "bnn_error = np.mean(np.square(X_test_with_anomalies - bnn_pred), axis=1)\n",
        "bnn_threshold = np.percentile(bnn_error, 95)\n",
        "bnn_anomalies = bnn_error > bnn_threshold\n",
        "\n",
        "# b) Convolutional Neural Network (CNN)\n",
        "def create_cnn_model(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Conv1D(32, kernel_size=2, activation='relu')(inputs)\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(16, activation='relu')(x)\n",
        "    # Change the output layer to match the input shape\n",
        "    outputs = layers.Dense(input_shape[0] * input_shape[1], activation='linear')(x) # or input_shape[0] * input_shape[1]\n",
        "    outputs = layers.Reshape(input_shape)(outputs)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "cnn_model = create_cnn_model((X_reshaped.shape[1], 1))\n",
        "cnn_model.compile(optimizer='adam', loss='mse')\n",
        "cnn_model.fit(X_train, X_train, epochs=20, batch_size=16, validation_split=0.2, verbose=0)\n",
        "cnn_pred = cnn_model.predict(X_test_with_anomalies)\n",
        "cnn_error = np.mean(np.square(X_test_with_anomalies - cnn_pred), axis=1)\n",
        "cnn_threshold = np.percentile(cnn_error, 95)\n",
        "cnn_anomalies = cnn_error > cnn_threshold\n",
        "\n",
        "# c) Recurrent Neural Network (RNN) with LSTM\n",
        "def create_rnn_model(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.LSTM(32, return_sequences=False)(inputs)\n",
        "    x = layers.Dense(16, activation='relu')(x)\n",
        "    # Reshape output to match X_test_with_anomalies shape\n",
        "    outputs = layers.Dense(input_shape[0] * input_shape[1], activation='linear')(x)\n",
        "    outputs = layers.Reshape(input_shape)(outputs)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "rnn_model = create_rnn_model((X_reshaped.shape[1], 1))\n",
        "rnn_model.compile(optimizer='adam', loss='mse')\n",
        "rnn_model.fit(X_train, X_train, epochs=20, batch_size=16, validation_split=0.2, verbose=0)\n",
        "rnn_pred = rnn_model.predict(X_test_with_anomalies)\n",
        "rnn_error = np.mean(np.square(X_test_with_anomalies - rnn_pred), axis=1)\n",
        "rnn_threshold = np.percentile(rnn_error, 95)\n",
        "rnn_anomalies = rnn_error > rnn_threshold\n",
        "\n",
        "# d) Autoencoder with GAN-like Anomaly Detection\n",
        "def create_autoencoder(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    encoded = layers.Conv1D(16, kernel_size=2, activation='relu', padding='same')(inputs)\n",
        "    decoded = layers.Conv1DTranspose(16, kernel_size=2, activation='relu', padding='same')(encoded)\n",
        "    outputs = layers.Conv1D(input_shape[1], kernel_size=3, activation='sigmoid', padding='same')(decoded)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "autoencoder = create_autoencoder((X_reshaped.shape[1], 1))\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "autoencoder.fit(X_train, X_train, epochs=20, batch_size=16, validation_split=0.2, verbose=0)\n",
        "ae_pred = autoencoder.predict(X_test_with_anomalies)\n",
        "ae_error = np.mean(np.square(X_test_with_anomalies - ae_pred), axis=1)\n",
        "ae_threshold = np.percentile(ae_error, 95)\n",
        "ae_anomalies = ae_error > ae_threshold\n",
        "\n",
        "\n",
        "# Combine Predictions (Ensemble)\n",
        "ensemble_pred = (bnn_pred + cnn_pred + rnn_pred + ae_pred) / 4\n",
        "ensemble_error = np.mean(np.square(X_test_with_anomalies - ensemble_pred), axis=1)\n",
        "ensemble_threshold = np.percentile(ensemble_error, 95)\n",
        "ensemble_anomalies = ensemble_error > ensemble_threshold\n",
        "\n",
        "# Test Performance\n",
        "def evaluate_model(true_anomalies, predicted_anomalies):\n",
        "    true_pos = np.sum((true_anomalies == 1) & (predicted_anomalies == 1))\n",
        "    false_pos = np.sum((true_anomalies == 0) & (predicted_anomalies == 1))\n",
        "    false_neg = np.sum((true_anomalies == 1) & (predicted_anomalies == 0))\n",
        "    precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n",
        "    recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    return precision, recall, f1\n",
        "\n",
        "bnn_metrics = evaluate_model(test_anomalies, bnn_anomalies)\n",
        "cnn_metrics = evaluate_model(test_anomalies, cnn_anomalies)\n",
        "rnn_metrics = evaluate_model(test_anomalies, rnn_anomalies)\n",
        "ae_metrics = evaluate_model(test_anomalies, ae_anomalies)\n",
        "ensemble_metrics = evaluate_model(test_anomalies, ensemble_anomalies)\n",
        "\n",
        "# Additional Displayable Outputs\n",
        "# 1. Confusion Matrix Heatmap\n",
        "cm = confusion_matrix(test_anomalies, ensemble_anomalies)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])\n",
        "plt.title('Confusion Matrix for Ensemble Model', fontsize=14)\n",
        "plt.xlabel('Predicted', fontsize=12)\n",
        "plt.ylabel('Actual', fontsize=12)\n",
        "plt.savefig('confusion_matrix.png')\n",
        "\n",
        "# 2. Feature Importance (using Autoencoder weights)\n",
        "encoder = Model(autoencoder.input, autoencoder.layers[1].output)\n",
        "feature_importance = np.mean(np.abs(encoder.predict(X_reshaped)), axis=(0, 2))\n",
        "feature_names = features\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(feature_names, feature_importance)\n",
        "plt.title('Feature Importance in Latent Space', fontsize=14)\n",
        "plt.xlabel('Operational Risk Factors', fontsize=12)\n",
        "plt.ylabel('Importance Score', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.savefig('feature_importance.png')\n",
        "\n",
        "# 3. Latent Space Representation (from Autoencoder)\n",
        "latent_space = encoder.predict(X_reshaped)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(latent_space[:, 0, 0], latent_space[:, 0, 1], c='blue', label='Latent Points')\n",
        "plt.title('Latent Space Representation', fontsize=14)\n",
        "plt.xlabel('Latent Dimension 1', fontsize=12)\n",
        "plt.ylabel('Latent Dimension 2', fontsize=12)\n",
        "plt.legend(fontsize=12)\n",
        "plt.savefig('latent_space.png')\n",
        "\n",
        "# 4. Time-Series Anomaly Trend\n",
        "time_series = X_scaled[:, 0]  # Using Internal_fraud as an example\n",
        "anomaly_indices = np.where(ensemble_anomalies)[0]\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(len(time_series)), time_series, label='Internal Fraud', color='blue')\n",
        "plt.scatter(anomaly_indices, time_series[anomaly_indices], color='red', label='Detected Anomalies', zorder=5)\n",
        "plt.title('Time-Series Anomaly Trend (Internal Fraud)', fontsize=14)\n",
        "plt.xlabel('Sample Index', fontsize=12)\n",
        "plt.ylabel('Normalized Value', fontsize=12)\n",
        "plt.legend(fontsize=12)\n",
        "plt.savefig('time_series_anomaly.png')\n",
        "\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.scatter(range(len(ensemble_error)), ensemble_error, c='blue', label='Reconstruction Error')\n",
        "plt.axhline(y=ensemble_threshold, color='r', linestyle='--', label='Threshold')\n",
        "plt.title('Ensemble Anomaly Detection', fontsize=14)\n",
        "plt.xlabel('Test Sample Index', fontsize=12)\n",
        "plt.ylabel('Error', fontsize=12)\n",
        "plt.legend(fontsize=12)\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "# Convert ensemble_anomalies to a 1D array for color mapping\n",
        "colors = np.where(ensemble_anomalies.flatten(), 'red', 'green')\n",
        "plt.scatter(range(len(X_test_with_anomalies)), X_test_with_anomalies[:, 0, 0], c=colors, label='Anomalies' if np.any(ensemble_anomalies) else 'No Anomalies')\n",
        "plt.title('Test Data with Detected Anomalies', fontsize=14)\n",
        "plt.xlabel('Test Sample Index', fontsize=12)\n",
        "plt.ylabel('Normalized Value', fontsize=12)\n",
        "plt.legend(fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig('anomaly_results.png')\n",
        "\n",
        "\n",
        "\n",
        "print(\"BNN Metrics - Precision: {:.2f}, Recall: {:.2f}, F1: {:.2f}\".format(*bnn_metrics))\n",
        "print(\"CNN Metrics - Precision: {:.2f}, Recall: {:.2f}, F1: {:.2f}\".format(*cnn_metrics))\n",
        "print(\"RNN Metrics - Precision: {:.2f}, Recall: {:.2f}, F1: {:.2f}\".format(*rnn_metrics))\n",
        "print(\"Autoencoder Metrics - Precision: {:.2f}, Recall: {:.2f}, F1: {:.2f}\".format(*ae_metrics))\n",
        "print(\"Ensemble Metrics - Precision: {:.2f}, Recall: {:.2f}, F1: {:.2f}\".format(*ensemble_metrics))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVQ8iaXE_02Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Preprocess data\n",
        "data = opr_data\n",
        "features = ['Internal_fraud', 'External_fraud', 'Employ_PWS', 'Clients_PBP', 'Disaster', 'ICT_Failure', 'Execution_DP']\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(data[features])\n",
        "X_reshaped = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))\n",
        "\n",
        "# Synthetic anomalies for testing\n",
        "def add_synthetic_anomalies(data, anomaly_rate=0.05):\n",
        "    # Ensure anomaly_rate results in at least one anomaly\n",
        "    num_anomalies = int(data.shape[0] * anomaly_rate)\n",
        "    if num_anomalies == 0:\n",
        "        num_anomalies = 1  # Minimum 1 anomaly\n",
        "\n",
        "    anomalies = np.zeros(data.shape[0], dtype=int)\n",
        "    anomalies[:num_anomalies] = 1  # Mark first num_anomalies as anomalies\n",
        "    np.random.shuffle(anomalies)  # Randomize\n",
        "\n",
        "    anomaly_mask = anomalies == 1\n",
        "    data_with_anomalies = data.copy()\n",
        "\n",
        "    # Reshape noise to match data shape\n",
        "    noise_shape = (anomaly_mask.sum(), data.shape[1], data.shape[2])  # Correct shape\n",
        "    data_with_anomalies[anomaly_mask] += np.random.normal(0, 5, size=noise_shape)\n",
        "    return data_with_anomalies, anomaly_mask\n",
        "\n",
        "# Model definitions (simplified for brevity, same as before)\n",
        "def create_bnn_model(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Dense(32, activation='relu')(inputs)\n",
        "    x = layers.Dense(16, activation='relu')(x)\n",
        "    outputs = layers.Dense(input_shape[1], activation='linear')(x)\n",
        "    return Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "def create_cnn_model(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Conv1D(32, kernel_size=2, activation='relu')(inputs)\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(16, activation='relu')(x)\n",
        "    # Change the output layer to match the input shape\n",
        "    # outputs = layers.Dense(input_shape[1], activation='linear')(x)\n",
        "    outputs = layers.Dense(input_shape[0] * input_shape[1], activation='linear')(x)  # or input_shape[0] * input_shape[1]\n",
        "    outputs = layers.Reshape(input_shape)(outputs)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def create_rnn_model(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.LSTM(32, return_sequences=False)(inputs)\n",
        "    x = layers.Dense(16, activation='relu')(x)\n",
        "    # outputs = layers.Dense(input_shape[1], activation='linear')(x)\n",
        "    # Reshape output to match X_test_with_anomalies shape\n",
        "    outputs = layers.Dense(input_shape[0] * input_shape[1], activation='linear')(x)\n",
        "    outputs = layers.Reshape(input_shape)(outputs)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def create_autoencoder(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    encoded = layers.Conv1D(16, kernel_size=2, activation='relu', padding='same')(inputs)\n",
        "    decoded = layers.Conv1DTranspose(16, kernel_size=2, activation='relu', padding='same')(encoded)\n",
        "    outputs = layers.Conv1D(input_shape[1], kernel_size=3, activation='sigmoid', padding='same')(decoded)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# 1. K-Fold Cross-Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = []\n",
        "for fold, (train_idx, test_idx) in enumerate(kf.split(X_reshaped)):\n",
        "    X_train, X_test = X_reshaped[train_idx], X_reshaped[test_idx]\n",
        "    X_test_with_anomalies, test_anomalies = add_synthetic_anomalies(X_test)\n",
        "\n",
        "    # Train models\n",
        "    bnn_model = create_bnn_model((X_reshaped.shape[1], 1))\n",
        "    bnn_model.compile(optimizer='adam', loss='mse')\n",
        "    bnn_model.fit(X_train, X_train, epochs=20, batch_size=16, verbose=0)\n",
        "    bnn_pred = bnn_model.predict(X_test_with_anomalies)\n",
        "\n",
        "    cnn_model = create_cnn_model((X_reshaped.shape[1], 1))\n",
        "    cnn_model.compile(optimizer='adam', loss='mse')\n",
        "    cnn_model.fit(X_train, X_train, epochs=20, batch_size=16, verbose=0)\n",
        "    cnn_pred = cnn_model.predict(X_test_with_anomalies)\n",
        "\n",
        "    rnn_model = create_rnn_model((X_reshaped.shape[1], 1))\n",
        "    rnn_model.compile(optimizer='adam', loss='mse')\n",
        "    rnn_model.fit(X_train, X_train, epochs=20, batch_size=16, verbose=0)\n",
        "    rnn_pred = rnn_model.predict(X_test_with_anomalies)\n",
        "\n",
        "    autoencoder = create_autoencoder((X_reshaped.shape[1], 1))\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "    autoencoder.fit(X_train, X_train, epochs=20, batch_size=16, verbose=0)\n",
        "    ae_pred = autoencoder.predict(X_test_with_anomalies)\n",
        "\n",
        "    # Ensemble\n",
        "    ensemble_pred = (bnn_pred + cnn_pred + rnn_pred + ae_pred) / 4\n",
        "    ensemble_error = np.mean(np.square(X_test_with_anomalies - ensemble_pred), axis=1)\n",
        "    ensemble_threshold = np.percentile(ensemble_error, 95)\n",
        "    ensemble_anomalies = ensemble_error > ensemble_threshold\n",
        "\n",
        "    # Evaluate\n",
        "    true_pos = np.sum((test_anomalies == 1) & (ensemble_anomalies == 1))\n",
        "    false_pos = np.sum((test_anomalies == 0) & (ensemble_anomalies == 1))\n",
        "    false_neg = np.sum((test_anomalies == 1) & (ensemble_anomalies == 0))\n",
        "    precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n",
        "    recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    cv_scores.append(f1)\n",
        "\n",
        "print(f\"Cross-Validation F1 Scores: {cv_scores}\")\n",
        "print(f\"Average CV F1 Score: {np.mean(cv_scores):.2f}  {np.std(cv_scores):.2f}\")\n",
        "\n",
        "# 2. Real-World Scenario Simulation\n",
        "# Simulate a cybersecurity breach: Sudden spike in Internal_fraud and External_fraud\n",
        "X_simulated = X_reshaped.copy()\n",
        "simulated_anomalies = np.zeros(X_reshaped.shape[0], dtype=int)\n",
        "breach_indices = [10, 11, 12]  # Simulating a breach in 1990-1992\n",
        "simulated_anomalies[breach_indices] = 1\n",
        "X_simulated[breach_indices, 0, 0] += 5  # Spike in Internal_fraud\n",
        "X_simulated[breach_indices, 1, 0] += 5  # Spike in External_fraud\n",
        "\n",
        "# Predict with ensemble\n",
        "bnn_pred_sim = bnn_model.predict(X_simulated)\n",
        "cnn_pred_sim = cnn_model.predict(X_simulated)\n",
        "rnn_pred_sim = rnn_model.predict(X_simulated)\n",
        "ae_pred_sim = autoencoder.predict(X_simulated)\n",
        "ensemble_pred_sim = (bnn_pred_sim + cnn_pred_sim + rnn_pred_sim + ae_pred_sim) / 4\n",
        "ensemble_error_sim = np.mean(np.square(X_simulated - ensemble_pred_sim), axis=1)\n",
        "ensemble_threshold_sim = np.percentile(ensemble_error_sim, 95)\n",
        "ensemble_anomalies_sim = ensemble_error_sim > ensemble_threshold_sim\n",
        "\n",
        "# Evaluate simulation\n",
        "cm_sim = confusion_matrix(simulated_anomalies, ensemble_anomalies_sim)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_sim, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])\n",
        "plt.title('Confusion Matrix - Simulated Cybersecurity Breach', fontsize=14)\n",
        "plt.xlabel('Predicted', fontsize=12)\n",
        "plt.ylabel('Actual', fontsize=12)\n",
        "plt.savefig('confusion_matrix_simulated.png')\n",
        "\n",
        "# 3. Additional Outputs to Prove Usefulness\n",
        "# Split data for final evaluation\n",
        "X_train, X_test = X_reshaped[:36], X_reshaped[36:]\n",
        "X_test_with_anomalies, test_anomalies = add_synthetic_anomalies(X_test)\n",
        "\n",
        "# Train models on final split\n",
        "bnn_model = create_bnn_model((X_reshaped.shape[1], 1))\n",
        "bnn_model.compile(optimizer='adam', loss='mse')\n",
        "bnn_model.fit(X_train, X_train, epochs=20, batch_size=16, verbose=0)\n",
        "cnn_model = create_cnn_model((X_reshaped.shape[1], 1))\n",
        "cnn_model.compile(optimizer='adam', loss='mse')\n",
        "cnn_model.fit(X_train, X_train, epochs=20, batch_size=16, verbose=0)\n",
        "rnn_model = create_rnn_model((X_reshaped.shape[1], 1))\n",
        "rnn_model.compile(optimizer='adam', loss='mse')\n",
        "rnn_model.fit(X_train, X_train, epochs=20, batch_size=16, verbose=0)\n",
        "autoencoder = create_autoencoder((X_reshaped.shape[1], 1))\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "autoencoder.fit(X_train, X_train, epochs=20, batch_size=16, verbose=0)\n",
        "\n",
        "# Ensemble prediction\n",
        "bnn_pred = bnn_model.predict(X_test_with_anomalies)\n",
        "cnn_pred = cnn_model.predict(X_test_with_anomalies)\n",
        "rnn_pred = rnn_model.predict(X_test_with_anomalies)\n",
        "ae_pred = autoencoder.predict(X_test_with_anomalies)\n",
        "ensemble_pred = (bnn_pred + cnn_pred + rnn_pred + ae_pred) / 4\n",
        "ensemble_error = np.mean(np.square(X_test_with_anomalies - ensemble_pred), axis=1)\n",
        "ensemble_threshold = np.percentile(ensemble_error, 95)\n",
        "ensemble_anomalies = ensemble_error > ensemble_threshold\n",
        "\n",
        "# a) ROC Curve and AUC\n",
        "fpr, tpr, _ = roc_curve(test_anomalies, ensemble_error)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14)\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.legend(loc='lower right', fontsize=12)\n",
        "plt.savefig('roc_curve.png')\n",
        "\n",
        "# b) Precision-Recall Curve\n",
        "precision, recall, _ = precision_recall_curve(test_anomalies, ensemble_error)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')\n",
        "plt.title('Precision-Recall Curve', fontsize=14)\n",
        "plt.xlabel('Recall', fontsize=12)\n",
        "plt.ylabel('Precision', fontsize=12)\n",
        "plt.legend(loc='lower left', fontsize=12)\n",
        "plt.savefig('precision_recall_curve.png')\n",
        "\n",
        "# c) Risk Score Distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(ensemble_error[~test_anomalies], bins=20, alpha=0.5, color='green', label='Normal')\n",
        "plt.hist(ensemble_error[test_anomalies], bins=20, alpha=0.5, color='red', label='Anomalous')\n",
        "plt.title('Risk Score Distribution', fontsize=14)\n",
        "plt.xlabel('Reconstruction Error (Risk Score)', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.legend(fontsize=12)\n",
        "plt.savefig('risk_score_distribution.png')\n",
        "\n",
        "# d) Anomaly Case Study (for index 7 from previous results)\n",
        "anomaly_idx = 7\n",
        "if ensemble_anomalies[anomaly_idx]:\n",
        "    anomaly_features = scaler.inverse_transform(X_test_with_anomalies[anomaly_idx, :, 0].reshape(1, -1))\n",
        "    print(f\"Anomaly Case Study (Index {anomaly_idx}):\")\n",
        "    for i, feature in enumerate(features):\n",
        "        print(f\"{feature}: {anomaly_features[0, i]:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEYQuZ2E_0zC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbf3iLmltI1H"
      },
      "source": [
        "# **objective 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrnKr2QEtUou"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load dataset\n",
        "dataset = opr_data\n",
        "# Data Preprocessing\n",
        "features = ['Internal_fraud', 'External_fraud', 'Employ_PWS', 'Clients_PBP', 'Disaster', 'ICT_Failure', 'Execution_DP']\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(dataset[features])\n",
        "\n",
        "# Define Autoencoder Model\n",
        "input_dim = len(features)  # 7 features\n",
        "latent_dim = 2  # Latent space dimension\n",
        "\n",
        "# Encoder\n",
        "inputs = Input(shape=(input_dim,))\n",
        "h = Dense(16, activation='relu')(inputs)\n",
        "h = Dense(8, activation='relu')(h)\n",
        "z = Dense(latent_dim, activation='relu')(h)\n",
        "\n",
        "# Decoder\n",
        "h_decoded = Dense(8, activation='relu')(z)\n",
        "h_decoded = Dense(16, activation='relu')(h_decoded)\n",
        "outputs = Dense(input_dim, activation='linear')(h_decoded)\n",
        "\n",
        "# Autoencoder Model\n",
        "autoencoder = Model(inputs, outputs)\n",
        "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "# Train the Autoencoder\n",
        "history = autoencoder.fit(X_scaled, X_scaled, epochs=50, batch_size=16, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Plot Training and Validation Loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='green')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss (Autoencoder)')\n",
        "plt.legend()\n",
        "plt.savefig('autoencoder_loss.png')\n",
        "\n",
        "# Get Reconstruction Errors\n",
        "reconstructions = autoencoder.predict(X_scaled)\n",
        "reconstruction_errors = np.mean(np.square(X_scaled - reconstructions), axis=1)\n",
        "\n",
        "# Plot Reconstruction Error Distribution\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(reconstruction_errors, bins=20, color='blue', alpha=0.7, label='Reconstruction Errors')\n",
        "threshold = np.percentile(reconstruction_errors, 95)  # 95th percentile as threshold\n",
        "plt.axvline(threshold, color='red', linestyle='--', label=f'Threshold (95th percentile): {threshold:.2f}')\n",
        "plt.xlabel('Reconstruction Error')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Reconstruction Error Distribution')\n",
        "plt.legend()\n",
        "plt.savefig('reconstruction_error_distribution.png')\n",
        "\n",
        "# Identify Anomalies\n",
        "anomalies = reconstruction_errors > threshold\n",
        "dataset['Anomaly'] = anomalies\n",
        "anomaly_indices = dataset.index[anomalies].tolist()\n",
        "\n",
        "# Plot Latent Space Representation\n",
        "encoder = Model(inputs, z)\n",
        "latent_space = encoder.predict(X_scaled)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(latent_space[~anomalies, 0], latent_space[~anomalies, 1], color='green', label='Normal', alpha=0.6)\n",
        "plt.scatter(latent_space[anomalies, 0], latent_space[anomalies, 1], color='red', label='Anomaly', marker='x')\n",
        "plt.xlabel('Latent Dimension 1')\n",
        "plt.ylabel('Latent Dimension 2')\n",
        "plt.title('Latent Space Representation (Autoencoder)')\n",
        "plt.legend()\n",
        "plt.savefig('latent_space_representation.png')\n",
        "\n",
        "# Time-Series Anomaly Plot for a Key Feature (Internal_fraud)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(dataset['Date'], dataset['Internal_fraud'], label='Internal Fraud', color='blue')\n",
        "plt.scatter(dataset['Date'][anomalies], dataset['Internal_fraud'][anomalies], color='red', label='Detected Anomalies', marker='x')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Internal Fraud')\n",
        "plt.title('Time-Series Anomaly Detection (Internal Fraud)')\n",
        "plt.legend()\n",
        "plt.savefig('time_series_anomaly_internal_fraud.png')\n",
        "\n",
        "# Simulated Ground Truth for Evaluation (from Objective 1)\n",
        "# Known anomalies at indices 1012 (19901992)\n",
        "ground_truth = np.zeros(len(dataset))\n",
        "ground_truth[10:13] = 1  # Indices 10, 11, 12\n",
        "predicted = anomalies.astype(int)\n",
        "\n",
        "# Calculate Precision, Recall, F1-Score\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(ground_truth, predicted, average='binary')\n",
        "print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}\")\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(ground_truth, predicted)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Simulated Anomalies')\n",
        "plt.savefig('confusion_matrix_anomalies.png')\n",
        "\n",
        "# Anomaly Details Table\n",
        "anomaly_data = dataset[anomalies][['Date'] + features].copy()\n",
        "anomaly_data['Reconstruction Error'] = reconstruction_errors[anomalies]\n",
        "\n",
        "# Round numerical columns to integers to remove decimal commas\n",
        "numerical_columns = ['Date'] + features + ['Reconstruction Error']\n",
        "anomaly_data[numerical_columns] = anomaly_data[numerical_columns].round(0).astype(int)\n",
        "\n",
        "# Plot the table\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.axis('tight')\n",
        "plt.axis('off')\n",
        "table = plt.table(cellText=anomaly_data.values, colLabels=anomaly_data.columns, cellLoc='center', loc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(8)\n",
        "table.scale(1.2, 1.2)\n",
        "plt.title('Detected Anomalies Details')\n",
        "plt.savefig('anomaly_details_table.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "C6iOVatp6TVL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load dataset\n",
        "dataset = opr_data\n",
        "\n",
        "# Data Preprocessing\n",
        "features = ['Internal_fraud', 'External_fraud', 'Employ_PWS', 'Clients_PBP', 'Disaster', 'ICT_Failure', 'Execution_DP']\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(dataset[features])\n",
        "\n",
        "# Define Enhanced Autoencoder Model\n",
        "input_dim = len(features)  # 7 features\n",
        "latent_dim = 3  # Increased latent dimension\n",
        "\n",
        "# Encoder\n",
        "inputs = Input(shape=(input_dim,))\n",
        "h = Dense(32, activation='relu')(inputs)  # Increased units\n",
        "h = Dense(16, activation='relu')(h)\n",
        "z = Dense(latent_dim, activation='relu')(h)\n",
        "\n",
        "# Decoder\n",
        "h_decoded = Dense(16, activation='relu')(z)\n",
        "h_decoded = Dense(32, activation='relu')(h_decoded)\n",
        "outputs = Dense(input_dim, activation='linear')(h_decoded)\n",
        "\n",
        "# Autoencoder Model\n",
        "autoencoder = Model(inputs, outputs)\n",
        "autoencoder.compile(optimizer=RMSprop(learning_rate=0.0001), loss='mse')  # Adjusted optimizer and learning rate\n",
        "\n",
        "# Train the Autoencoder with Early Stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "history = autoencoder.fit(X_scaled, X_scaled, epochs=100, batch_size=16, validation_split=0.2,\n",
        "                         callbacks=[early_stopping], verbose=1)\n",
        "\n",
        "# Plot Training and Validation Loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='green')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss (Enhanced Autoencoder)')\n",
        "plt.legend()\n",
        "plt.savefig('autoencoder_loss_enhanced.png')\n",
        "\n",
        "# Get Reconstruction Errors\n",
        "reconstructions = autoencoder.predict(X_scaled)\n",
        "reconstruction_errors = np.mean(np.square(X_scaled - reconstructions), axis=1)\n",
        "\n",
        "# Plot Reconstruction Error Distribution\n",
        "threshold = np.percentile(reconstruction_errors, 90)  # Lowered to 90th percentile\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(reconstruction_errors, bins=20, color='blue', alpha=0.7, label='Reconstruction Errors')\n",
        "plt.axvline(threshold, color='red', linestyle='--', label=f'Threshold (90th percentile): {threshold:.2f}')\n",
        "plt.xlabel('Reconstruction Error')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Reconstruction Error Distribution')\n",
        "plt.legend()\n",
        "plt.savefig('reconstruction_error_distribution_enhanced.png')\n",
        "\n",
        "# Identify Anomalies\n",
        "anomalies = reconstruction_errors > threshold\n",
        "dataset['Anomaly'] = anomalies\n",
        "anomaly_indices = dataset.index[anomalies].tolist()\n",
        "\n",
        "# Plot Latent Space Representation\n",
        "encoder = Model(inputs, z)\n",
        "latent_space = encoder.predict(X_scaled)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(latent_space[~anomalies, 0], latent_space[~anomalies, 1], color='green', label='Normal', alpha=0.6)\n",
        "plt.scatter(latent_space[anomalies, 0], latent_space[anomalies, 1], color='red', label='Anomaly', marker='x')\n",
        "plt.xlabel('Latent Dimension 1')\n",
        "plt.ylabel('Latent Dimension 2')\n",
        "plt.title('Latent Space Representation (Enhanced Autoencoder)')\n",
        "plt.legend()\n",
        "plt.savefig('latent_space_representation_enhanced.png')\n",
        "\n",
        "# Time-Series Anomaly Plot for All Variables\n",
        "fig, axes = plt.subplots(nrows=len(features), ncols=1, figsize=(12, 4 * len(features)), sharex=True)\n",
        "for i, feature in enumerate(features):\n",
        "    ax = axes[i] if len(features) > 1 else axes\n",
        "    ax.plot(dataset['Date'], dataset[feature], label=feature, color='blue')\n",
        "    ax.scatter(dataset['Date'][anomalies], dataset[feature][anomalies], color='red', label='Detected Anomalies', marker='x')\n",
        "    ax.set_ylabel(feature)\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "axes[-1].set_xlabel('Year')\n",
        "fig.suptitle('Time-Series Anomaly Detection for All Variables', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('time_series_anomaly_all_variables_enhanced.png')\n",
        "\n",
        "# Simulated Ground Truth for Evaluation\n",
        "ground_truth = np.zeros(len(dataset))\n",
        "ground_truth[10:13] = 1  # Indices 10, 11, 12\n",
        "predicted = anomalies.astype(int)\n",
        "\n",
        "# Calculate Precision, Recall, F1-Score\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(ground_truth, predicted, average='binary', zero_division=0)\n",
        "print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}\")\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(ground_truth, predicted)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Simulated Anomalies')\n",
        "plt.savefig('confusion_matrix_anomalies_enhanced.png')\n",
        "\n",
        "# Anomaly Details Table\n",
        "anomaly_data = dataset[anomalies][['Date'] + features].copy()\n",
        "anomaly_data['Reconstruction Error'] = reconstruction_errors[anomalies]\n",
        "numerical_columns = ['Date'] + features + ['Reconstruction Error']\n",
        "anomaly_data[numerical_columns] = anomaly_data[numerical_columns].round(0).astype(int)\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.axis('tight')\n",
        "plt.axis('off')\n",
        "table = plt.table(cellText=anomaly_data.values, colLabels=anomaly_data.columns, cellLoc='center', loc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(8)\n",
        "table.scale(1.2, 1.2)\n",
        "plt.title('Detected Anomalies Details')\n",
        "plt.savefig('anomaly_details_table_enhanced.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhRZzwyA9iMk"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset = opr_data\n",
        "\n",
        "# Data Preprocessing\n",
        "features = ['Internal_fraud', 'External_fraud', 'Employ_PWS', 'Clients_PBP', 'Disaster', 'ICT_Failure', 'Execution_DP']\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(dataset[features])\n",
        "\n",
        "# Add Temporal Features (Moving Averages)\n",
        "window_size = 3  # 3-year moving average\n",
        "for feature in features:\n",
        "    dataset[f'{feature}_MA'] = dataset[feature].rolling(window=window_size, min_periods=1).mean()\n",
        "temporal_features = [f'{f}_MA' for f in features]\n",
        "X_scaled_temporal = scaler.fit_transform(dataset[features + temporal_features])\n",
        "\n",
        "# Define Enhanced Autoencoder Model\n",
        "input_dim = len(features + temporal_features)  # 14 features (7 original + 7 moving averages)\n",
        "latent_dim = 3  # Increased latent dimension\n",
        "\n",
        "# Encoder\n",
        "inputs = Input(shape=(input_dim,))\n",
        "h = Dense(64, activation='relu')(inputs)  # Increased units for more complex data\n",
        "h = Dense(32, activation='relu')(h)\n",
        "z = Dense(latent_dim, activation='relu')(h)\n",
        "\n",
        "# Decoder\n",
        "h_decoded = Dense(32, activation='relu')(z)\n",
        "h_decoded = Dense(64, activation='relu')(h_decoded)\n",
        "outputs = Dense(input_dim, activation='linear')(h_decoded)\n",
        "\n",
        "# Autoencoder Model\n",
        "autoencoder = Model(inputs, outputs)\n",
        "autoencoder.compile(optimizer=RMSprop(learning_rate=0.001), loss='mse')\n",
        "\n",
        "# Train the Autoencoder with Early Stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "history = autoencoder.fit(X_scaled_temporal, X_scaled_temporal, epochs=100, batch_size=16, validation_split=0.2,\n",
        "                         callbacks=[early_stopping], verbose=1)\n",
        "\n",
        "# Plot Training and Validation Loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='green')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss (Enhanced Autoencoder)')\n",
        "plt.legend()\n",
        "plt.savefig('autoencoder_loss_enhanced.png')\n",
        "\n",
        "# Get Reconstruction Errors\n",
        "reconstructions = autoencoder.predict(X_scaled_temporal)\n",
        "reconstruction_errors = np.mean(np.square(X_scaled_temporal - reconstructions), axis=1)\n",
        "\n",
        "# Plot Reconstruction Error Distribution\n",
        "threshold = np.percentile(reconstruction_errors, 89)  # Adjusted to 85th percentile for better recall\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(reconstruction_errors, bins=20, color='blue', alpha=0.7, label='Reconstruction Errors')\n",
        "plt.axvline(threshold, color='red', linestyle='--', label=f'Threshold (85th percentile): {threshold:.2f}')\n",
        "plt.xlabel('Reconstruction Error')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Reconstruction Error Distribution')\n",
        "plt.legend()\n",
        "plt.savefig('reconstruction_error_distribution_enhanced.png')\n",
        "\n",
        "# Identify Anomalies\n",
        "anomalies = reconstruction_errors > threshold\n",
        "dataset['Anomaly'] = anomalies\n",
        "anomaly_indices = dataset.index[anomalies].tolist()\n",
        "\n",
        "# Plot Latent Space Representation\n",
        "encoder = Model(inputs, z)\n",
        "latent_space = encoder.predict(X_scaled_temporal)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(latent_space[~anomalies, 0], latent_space[~anomalies, 1], color='green', label='Normal', alpha=0.6)\n",
        "plt.scatter(latent_space[anomalies, 0], latent_space[anomalies, 1], color='red', label='Anomaly', marker='x')\n",
        "plt.xlabel('Latent Dimension 1')\n",
        "plt.ylabel('Latent Dimension 2')\n",
        "plt.title('Latent Space Representation (Enhanced Autoencoder)')\n",
        "plt.legend()\n",
        "plt.savefig('latent_space_representation_enhanced.png')\n",
        "\n",
        "# Time-Series Anomaly Plot for All Variables\n",
        "fig, axes = plt.subplots(nrows=len(features), ncols=1, figsize=(12, 4 * len(features)), sharex=True)\n",
        "for i, feature in enumerate(features):\n",
        "    ax = axes[i] if len(features) > 1 else axes\n",
        "    ax.plot(dataset['Date'], dataset[feature], label=feature, color='blue')\n",
        "    ax.scatter(dataset['Date'][anomalies], dataset[feature][anomalies], color='red', label='Detected Anomalies', marker='x')\n",
        "    ax.set_ylabel(feature)\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "axes[-1].set_xlabel('Year')\n",
        "fig.suptitle('Time-Series Anomaly Detection for All Variables', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('time_series_anomaly_all_variables_enhanced.png')\n",
        "\n",
        "# Enhanced Simulated Ground Truth for Evaluation (Based on Historical Peaks)\n",
        "ground_truth = np.zeros(len(dataset))\n",
        "# Simulate anomalies at known peaks (e.g., 1982, 1993, 2003 from time-series plots)\n",
        "ground_truth[dataset['Date'] == 1982] = 1\n",
        "ground_truth[dataset['Date'] == 1993] = 1\n",
        "ground_truth[dataset['Date'] == 2003] = 1\n",
        "predicted = anomalies.astype(int)\n",
        "\n",
        "# Calculate Precision, Recall, F1-Score\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(ground_truth, predicted, average='binary', zero_division=0)\n",
        "print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}\")\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "cm = confusion_matrix(ground_truth, predicted)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Simulated Anomalies')\n",
        "plt.savefig('confusion_matrix_anomalies_enhanced.png')\n",
        "\n",
        "# Anomaly Details Table\n",
        "anomaly_data = dataset[anomalies][['Date'] + features].copy()\n",
        "anomaly_data['Reconstruction Error'] = reconstruction_errors[anomalies]\n",
        "numerical_columns = ['Date'] + features + ['Reconstruction Error']\n",
        "anomaly_data[numerical_columns] = anomaly_data[numerical_columns].round(0).astype(int)\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.axis('tight')\n",
        "plt.axis('off')\n",
        "table = plt.table(cellText=anomaly_data.values, colLabels=anomaly_data.columns, cellLoc='center', loc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(8)\n",
        "table.scale(1.2, 1.2)\n",
        "plt.title('Detected Anomalies Details')\n",
        "plt.savefig('anomaly_details_table_enhanced.png')\n",
        "\n",
        "# Note: For domain validation, share the anomaly table with risk experts to confirm if flagged events (e.g., 1993 fraud spike) correspond to actual operational risks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st2Y7NMFT2uQ"
      },
      "source": [
        "# **Objective 4**\n",
        "**4.\tApply Bayesian deep learning approaches to measure uncertainty and confidence levels in risk assessments.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDPh4haCV1QK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Load and preprocess data\n",
        "\n",
        "df = opr_data\n",
        "\n",
        "# Handle missing values in Disaster (impute with median)\n",
        "df['Disaster'].fillna(df['Disaster'].median(), inplace=True)\n",
        "\n",
        "# Prepare features and target\n",
        "X = df[['Internal_fraud', 'External_fraud', 'Employ_PWS', 'Clients_PBP', 'Disaster', 'ICT_Failure', 'Execution_DP']].values\n",
        "y = df['loss'].values\n",
        "\n",
        "# Scale features and target\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Split data (small dataset, so use 80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build Bayesian Neural Network with Monte Carlo Dropout\n",
        "def build_bnn_model(input_dim, dropout_rate=0.3):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
        "        tf.keras.layers.Dropout(dropout_rate),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dropout(dropout_rate),\n",
        "        tf.keras.layers.Dense(1)  # Regression output\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# Monte Carlo Dropout for uncertainty estimation\n",
        "def predict_with_uncertainty(model, X, n_iter=100):\n",
        "    preds = []\n",
        "    for _ in range(n_iter):\n",
        "        pred = model(X, training=True)  # Keep dropout active\n",
        "        preds.append(pred.numpy())\n",
        "    preds = np.array(preds)  # Shape: (n_iter, n_samples, 1)\n",
        "    mean_pred = np.mean(preds, axis=0)  # Mean prediction\n",
        "    std_pred = np.std(preds, axis=0)   # Standard deviation (uncertainty)\n",
        "    return mean_pred, std_pred\n",
        "\n",
        "# Build and train model\n",
        "model = build_bnn_model(input_dim=X_train.shape[1])\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=8, verbose=0)\n",
        "\n",
        "# Predict with uncertainty\n",
        "mean_pred_scaled, std_pred_scaled = predict_with_uncertainty(model, X_test)\n",
        "\n",
        "# Inverse scale predictions and uncertainty\n",
        "mean_pred = scaler_y.inverse_transform(mean_pred_scaled).flatten()\n",
        "# Reshape std_pred to be 1D\n",
        "std_pred = (std_pred_scaled * scaler_y.scale_).flatten()  # Scale std by target scale and flatten\n",
        "y_test_orig = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Visualize results\n",
        "plt.figure(figsize=(10, 6))\n",
        "indices = np.arange(len(y_test))\n",
        "plt.errorbar(indices, mean_pred, yerr=2*std_pred, fmt='o', label='Predicted Loss (2)', alpha=0.6)\n",
        "plt.scatter(indices, y_test_orig, color='red', label='True Loss', s=50)\n",
        "plt.xlabel('Test Sample Index')\n",
        "plt.ylabel('Operational Loss')\n",
        "plt.title('Risk Assessment with Uncertainty (Bayesian Neural Network)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Save plot\n",
        "plt.savefig('risk_uncertainty_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# Print sample results\n",
        "print(\"Sample Predictions with Uncertainty:\")\n",
        "test_years = df.iloc[np.where(np.isin(y_scaled, y_test))[0]]['Date'].values\n",
        "for i in range(len(mean_pred)):\n",
        "    print(f\"Year {test_years[i]}: Predicted Loss = {mean_pred[i]:,.0f}, \"\n",
        "          f\"Uncertainty (2) = {2*std_pred[i]:,.0f}, True Loss = {y_test_orig[i]:,.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LlXH3iv_Kk5t"
      },
      "outputs": [],
      "source": [
        "!pip install scikeras\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install --upgrade scikeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgrA371TaEly"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.inspection import partial_dependence\n",
        "from scipy.stats import norm\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Load and preprocess data\n",
        "data = {\n",
        "    'Date': range(1980, 2021),\n",
        "    'loss': [36019, 29373, 38563, 34344, 31824, 29446, 27799, 32177, 37291, 33248, 34456, 29882, 42634, 43208, 39154, 27706, 34207, 40715, 37157, 46352, 31255, 46837, 50473, 29197, 27457, 38400, 48796, 25265, 31551, 34062, 20702, 25415, 27929, 30749, 32505, 38202, 46562, 28497, 39719, 36622, 33370],\n",
        "    'Internal_fraud': [1750, 1893, 2258, 2238, 536, 844, 1861, 1860, 1861, 4197, 1900, 2295, 2168, 1956, 1279, 2031, 959, 1334, 1160, 1557, 3120, 193, 1980, 371, 1170, 2262, 1545, 746, 999, 1976, 989, 1652, 1532, 1044, 3001, 1944, 100, 1631, 1037, 2097, 945],\n",
        "    'External_fraud': [724, 1520, 2004, 2038, 1640, 2498, 1146, 1886, 2096, 2412, 2569, 1100, 773, 3022, 2266, 1401, 3241, 2093, 2943, 2054, 3649, 3404, 1801, 2777, 2516, 3095, 1228, 2549, 2847, 593, 1053, 369, 1784, 2574, 3202, 2059, 3303, 896, 637, 1956, 2307],\n",
        "    'Employ_PWS': [1654, 647, 1722, 2013, 1448, 2326, 736, 453, 133, 2098, 1593, 1167, 1368, 525, 2667, 1278, 1266, 1635, 1489, 1334, 726, 1483, 2329, 2007, 2156, 893, 606, 1125, 1233, 1857, 185, 2118, 1105, 944, 593, 207, 1694, 1244, 426, 423, 999],\n",
        "    'Clients_PBP': [21568, 11388, 15673, 11764, 11959, 12836, 16555, 11649, 23790, 8738, 13692, 11922, 25129, 16376, 22223, 4601, 16869, 21227, 15576, 22458, 11379, 24865, 31092, 12460, 11881, 25174, 26057, 11340, 12059, 13028, 5589, 8569, 7971, 9625, 14757, 16640, 25854, 8012, 21890, 13502, 14654],\n",
        "    'Disaster': [212, 174, 105, 48, 156, 131, 172, 151, 184, 57, 205, 116, 266, 45, 239, 116, 48, 61, 74, 134, 142, 54, 98, 271, 238, 135, 103, 110, 149, 18, 79, np.nan, 132, 152, 61, 226, 2, np.nan, 118, 184, 235],\n",
        "    'ICT_Failure': [5250, 4141, 5245, 6087, 5098, 5081, 3596, 3674, 2195, 8499, 2512, 3614, 3563, 6790, 4410, 7495, 3653, 5558, 3329, 9290, 2625, 5620, 6268, 5828, 4629, 4740, 5088, 4706, 6928, 9421, 3885, 2260, 4823, 10159, 3393, 8278, 8355, 3893, 6138, 8257, 4242],\n",
        "    'Execution_DP': [4859, 9610, 11557, 10157, 10988, 5730, 3735, 12504, 7032, 7248, 11985, 9669, 9368, 14495, 6069, 10784, 8171, 8806, 12585, 9524, 9615, 11218, 6905, 5482, 4866, 2101, 14169, 4690, 7336, 7170, 8922, 10447, 10581, 6251, 7499, 8848, 7254, 12822, 9473, 10205, 9989]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Handle missing values in Disaster\n",
        "df['Disaster'] = df['Disaster'].fillna(df['Disaster'].median())\n",
        "\n",
        "# Prepare features and target\n",
        "X = df[['Internal_fraud', 'External_fraud', 'Employ_PWS', 'Clients_PBP', 'Disaster', 'ICT_Failure', 'Execution_DP']].values\n",
        "y = df['loss'].values\n",
        "\n",
        "# Scale features and target\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Correlation Analysis\n",
        "corr_matrix = df[['Internal_fraud', 'External_fraud', 'Employ_PWS', 'Clients_PBP', 'Disaster', 'ICT_Failure', 'Execution_DP', 'loss']].corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
        "plt.title('Correlation Matrix of Features and Loss')\n",
        "plt.tight_layout()\n",
        "plt.savefig('correlation_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# Build Bayesian Neural Network with Aleatoric and Epistemic Uncertainty\n",
        "def build_bnn_model(input_dim, dropout_rate=0.3):\n",
        "    inputs = tf.keras.Input(shape=(input_dim,))\n",
        "    x = tf.keras.layers.Dense(64, activation='relu')(inputs)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "    x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "    mu = tf.keras.layers.Dense(1)(x)\n",
        "    log_sigma = tf.keras.layers.Dense(1)(x)\n",
        "    outputs = tf.keras.layers.Concatenate()([mu, log_sigma])\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    def negloglik(y_true, y_pred):\n",
        "        mu, log_sigma = y_pred[:, 0], y_pred[:, 1]\n",
        "        dist = tfp.distributions.Normal(loc=mu, scale=tf.exp(log_sigma) + 1e-6)\n",
        "        return -tf.reduce_mean(dist.log_prob(y_true))\n",
        "\n",
        "    def custom_mae(y_true, y_pred):\n",
        "        mu = y_pred[:, 0]\n",
        "        return tf.reduce_mean(tf.abs(y_true - mu))\n",
        "\n",
        "    model.compile(optimizer='adam', loss=negloglik, metrics=[custom_mae])\n",
        "    return model\n",
        "\n",
        "# Monte Carlo Dropout for uncertainty estimation\n",
        "def predict_with_uncertainty(model, X, n_iter=100):\n",
        "    means, log_sigmas = [], []\n",
        "    for _ in range(n_iter):\n",
        "        pred = model(X, training=True)\n",
        "        means.append(pred[:, 0].numpy())\n",
        "        log_sigmas.append(pred[:, 1].numpy())\n",
        "    means = np.array(means)\n",
        "    sigmas = np.exp(np.array(log_sigmas))\n",
        "\n",
        "    mean_pred = np.mean(means, axis=0)\n",
        "    epistemic_var = np.var(means, axis=0)\n",
        "    aleatoric_var = np.mean(sigmas**2, axis=0)\n",
        "    total_var = epistemic_var + aleatoric_var\n",
        "    return mean_pred, np.sqrt(epistemic_var), np.sqrt(total_var)\n",
        "\n",
        "# Cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "rmse_scores, coverage_scores = [], []\n",
        "mean_preds, epistemic_stds, total_stds, y_tests, test_years_all = [], [], [], [], []\n",
        "\n",
        "for train_idx, test_idx in kf.split(X_scaled):\n",
        "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
        "    y_train, y_test = y_scaled[train_idx], y_scaled[test_idx]\n",
        "\n",
        "    model = build_bnn_model(input_dim=X_train.shape[1])\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=8, verbose=0)\n",
        "\n",
        "    mean_pred_scaled, epistemic_std_scaled, total_std_scaled = predict_with_uncertainty(model, X_test)\n",
        "\n",
        "    mean_pred = scaler_y.inverse_transform(mean_pred_scaled.reshape(-1, 1)).flatten()\n",
        "    epistemic_std = epistemic_std_scaled * scaler_y.scale_[0]\n",
        "    total_std = total_std_scaled * scaler_y.scale_[0]\n",
        "    y_test_orig = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "    mean_preds.extend(mean_pred)\n",
        "    epistemic_stds.extend(epistemic_std)\n",
        "    total_stds.extend(total_std)\n",
        "    y_tests.extend(y_test_orig)\n",
        "    test_years_all.extend(df.iloc[test_idx]['Date'].values)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_orig, mean_pred))\n",
        "    coverage_95 = np.mean((y_test_orig >= mean_pred - 2*total_std) & (y_test_orig <= mean_pred + 2*total_std))\n",
        "    rmse_scores.append(rmse)\n",
        "    coverage_scores.append(coverage_95)\n",
        "\n",
        "# Convert to arrays and sort by year\n",
        "mean_preds = np.array(mean_preds)\n",
        "epistemic_stds = np.array(epistemic_stds)\n",
        "total_stds = np.array(total_stds)\n",
        "y_tests = np.array(y_tests)\n",
        "test_years_all = np.array(test_years_all)\n",
        "sort_idx = np.argsort(test_years_all)\n",
        "mean_preds = mean_preds[sort_idx]\n",
        "epistemic_stds = epistemic_stds[sort_idx]\n",
        "total_stds = total_stds[sort_idx]\n",
        "y_tests = y_tests[sort_idx]\n",
        "test_years_all = test_years_all[sort_idx]\n",
        "\n",
        "# Feature Importance\n",
        "def compute_permutation_importance(model, X, y, n_iter=10):\n",
        "    baseline_pred = model(X, training=False)[:, 0].numpy()\n",
        "    baseline_mse = mean_squared_error(y, baseline_pred)\n",
        "    importance = []\n",
        "    for col in range(X.shape[1]):\n",
        "        mse_diff = []\n",
        "        for _ in range(n_iter):\n",
        "            X_permuted = X.copy()\n",
        "            np.random.shuffle(X_permuted[:, col])\n",
        "            pred_permuted = model(X_permuted, training=False)[:, 0].numpy()\n",
        "            mse_permuted = mean_squared_error(y, pred_permuted)\n",
        "            mse_diff.append(mse_permuted - baseline_mse)\n",
        "        importance.append(np.mean(mse_diff))\n",
        "    return np.array(importance)\n",
        "\n",
        "importance = compute_permutation_importance(model, X_scaled, y_scaled)\n",
        "feature_names = ['Internal_fraud', 'External_fraud', 'Employ_PWS', 'Clients_PBP', 'Disaster', 'ICT_Failure', 'Execution_DP']\n",
        "\n",
        "# Create Table for Feature Effects\n",
        "feature_effects = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Permutation_Importance': importance,\n",
        "    'Correlation_with_Loss': corr_matrix['loss'][feature_names].values\n",
        "})\n",
        "feature_effects.to_csv('feature_effects_table.csv', index=False)\n",
        "\n",
        "# Print Table\n",
        "print(\"\\nFeature Effects on Loss:\")\n",
        "print(feature_effects.to_string(index=False))\n",
        "\n",
        "# Partial Dependence Plots\n",
        "def wrapped_model():\n",
        "    return build_bnn_model(input_dim=X_scaled.shape[1])\n",
        "\n",
        "sklearn_model = KerasRegressor(model=wrapped_model, epochs=100, batch_size=8, verbose=0)\n",
        "sklearn_model.fit(X_scaled, y_scaled)\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, feature in enumerate(feature_names):\n",
        "    pd_results = partial_dependence(\n",
        "        estimator=sklearn_model,\n",
        "        X=X_scaled,\n",
        "        features=[i],\n",
        "        response_method='auto',\n",
        "        grid_resolution=50\n",
        "    )\n",
        "    grid_values = scaler_X.inverse_transform(\n",
        "        np.repeat(pd_results['grid_values'][0].reshape(-1, 1), X_scaled.shape[1], axis=1)\n",
        "    )[:, i]\n",
        "    pd_values = scaler_y.inverse_transform(pd_results['partial_dependence'][0].reshape(-1, 1)).flatten()\n",
        "    plt.subplot(3, 3, i+1)\n",
        "    plt.plot(grid_values, pd_values, 'b-')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Predicted Loss')\n",
        "    plt.title(f'PDP for {feature}')\n",
        "    plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('partial_dependence_plots.png')\n",
        "plt.close()\n",
        "\n",
        "# 1. Enhanced Main Plot with Confidence and Prediction Intervals\n",
        "plt.figure(figsize=(14, 8))\n",
        "indices = np.arange(len(mean_preds))\n",
        "plt.errorbar(indices, mean_preds, yerr=2*total_stds, fmt='o', label='Predicted Loss (95% Prediction Interval)', alpha=0.6)\n",
        "plt.fill_between(indices, mean_preds - 2*epistemic_stds, mean_preds + 2*epistemic_stds,\n",
        "                 color='blue', alpha=0.2, label='95% Confidence Interval (Epistemic)')\n",
        "plt.scatter(indices, y_tests, color='red', label='True Loss', s=50)\n",
        "threshold = 40000\n",
        "high_risk = mean_preds > threshold\n",
        "plt.scatter(indices[high_risk], mean_preds[high_risk], edgecolors='black', facecolors='none', s=100, label='High Risk (>40,000)')\n",
        "plt.xticks(indices[::2], test_years_all[::2], rotation=45)\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Operational Loss')\n",
        "plt.title('Risk Assessment with Confidence and Prediction Intervals (Cross-Validated)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('enhanced_risk_uncertainty_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# 2. Calibration Plot\n",
        "def calibration_plot(mean_pred, total_std, y_true):\n",
        "    coverage_probs = np.linspace(0, 1, 20)\n",
        "    observed_coverage = []\n",
        "    for p in coverage_probs:\n",
        "        z = norm.ppf(1 - (1 - p) / 2)\n",
        "        lower = mean_pred - z * total_std\n",
        "        upper = mean_pred + z * total_std\n",
        "        covered = np.mean((y_true >= lower) & (y_true <= upper))\n",
        "        observed_coverage.append(covered)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(coverage_probs, observed_coverage, marker='o', label='Observed Coverage')\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
        "    plt.xlabel('Expected Coverage Probability')\n",
        "    plt.ylabel('Observed Coverage Probability')\n",
        "    plt.title('Calibration of Uncertainty Estimates')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('calibration_plot.png')\n",
        "    plt.close()\n",
        "    return observed_coverage, coverage_probs\n",
        "\n",
        "observed_coverage, coverage_probs = calibration_plot(mean_preds, total_stds, y_tests)\n",
        "\n",
        "# 3. Feature Importance Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(feature_names, importance)\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Increase in MSE')\n",
        "plt.title('Feature Importance (Permutation)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# 4. Loss Distribution for a High-Risk Sample\n",
        "high_risk_idx = np.where(high_risk)[0][0] if np.any(high_risk) else 0\n",
        "sample_means = scaler_y.inverse_transform(np.array([model(X_scaled[sort_idx[high_risk_idx]:sort_idx[high_risk_idx]+1], training=True)[:, 0].numpy()\n",
        "                                                   for _ in range(100)]).reshape(-1, 1)).flatten()\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(sample_means, bins=30, density=True, alpha=0.7, label='Predicted Loss Distribution')\n",
        "plt.axvline(y_tests[high_risk_idx], color='red', linestyle='--', label='True Loss')\n",
        "plt.xlabel('Operational Loss')\n",
        "plt.ylabel('Density')\n",
        "plt.title(f'Loss Distribution for Year {test_years_all[high_risk_idx]}')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('loss_distribution_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# 5. Prediction Interval Coverage Over Time\n",
        "coverage_by_year = (y_tests >= mean_preds - 2*total_stds) & (y_tests <= mean_preds + 2*total_stds)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(test_years_all, coverage_by_year, c=coverage_by_year, cmap='RdYlGn', s=100, label='Within 95% Interval')\n",
        "plt.axhline(y=0.95, color='black', linestyle='--', label='Expected 95% Coverage')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Within 95% Prediction Interval')\n",
        "plt.title('Prediction Interval Coverage by Year')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('coverage_over_time_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# 6. Uncertainty Decomposition\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(indices, epistemic_stds**2, label='Epistemic Variance', alpha=0.5)\n",
        "plt.bar(indices, total_stds**2 - epistemic_stds**2, bottom=epistemic_stds**2, label='Aleatoric Variance', alpha=0.5)\n",
        "plt.xticks(indices[::2], test_years_all[::2], rotation=45)\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Variance')\n",
        "plt.title('Uncertainty Decomposition (Epistemic vs. Aleatoric)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('uncertainty_decomposition_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# Print results\n",
        "print(\"Cross-Validation Results:\")\n",
        "print(f\"Average RMSE: {np.mean(rmse_scores):,.0f}  {np.std(rmse_scores):,.0f}\")\n",
        "print(f\"Average 95% Prediction Interval Coverage: {np.mean(coverage_scores):.2%}  {np.std(coverage_scores):.2%}\")\n",
        "print(\"\\nSample Predictions with Uncertainty:\")\n",
        "for i in range(min(10, len(mean_preds))):\n",
        "    print(f\"Year {test_years_all[i]}: Predicted Loss = {mean_preds[i]:,.0f}, \"\n",
        "          f\"Epistemic Uncertainty (2) = {2*epistemic_stds[i]:,.0f}, \"\n",
        "          f\"Total Uncertainty (2) = {2*total_stds[i]:,.0f}, True Loss = {y_tests[i]:,.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m5_1RMQ6EV0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.inspection import partial_dependence, permutation_importance\n",
        "from scipy.stats import norm\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Load and preprocess data\n",
        "data = {\n",
        "    'Date': range(1980, 2021),\n",
        "    'loss': [36019, 29373, 38563, 34344, 31824, 29446, 27799, 32177, 37291, 33248, 34456, 29882, 42634, 43208, 39154, 27706, 34207, 40715, 37157, 46352, 31255, 46837, 50473, 29197, 27457, 38400, 48796, 25265, 31551, 34062, 20702, 25415, 27929, 30749, 32505, 38202, 46562, 28497, 39719, 36622, 33370],\n",
        "    'Internal_fraud': [1750, 1893, 2258, 2238, 536, 844, 1861, 1860, 1861, 4197, 1900, 2295, 2168, 1956, 1279, 2031, 959, 1334, 1160, 1557, 3120, 193, 1980, 371, 1170, 2262, 1545, 746, 999, 1976, 989, 1652, 1532, 1044, 3001, 1944, 100, 1631, 1037, 2097, 945],\n",
        "    'External_fraud': [724, 1520, 2004, 2038, 1640, 2498, 1146, 1886, 2096, 2412, 2569, 1100, 773, 3022, 2266, 1401, 3241, 2093, 2943, 2054, 3649, 3404, 1801, 2777, 2516, 3095, 1228, 2549, 2847, 593, 1053, 369, 1784, 2574, 3202, 2059, 3303, 896, 637, 1956, 2307],\n",
        "    'Employ_PWS': [1654, 647, 1722, 2013, 1448, 2326, 736, 453, 133, 2098, 1593, 1167, 1368, 525, 2667, 1278, 1266, 1635, 1489, 1334, 726, 1483, 2329, 2007, 2156, 893, 606, 1125, 1233, 1857, 185, 2118, 1105, 944, 593, 207, 1694, 1244, 426, 423, 999],\n",
        "    'Clients_PBP': [21568, 11388, 15673, 11764, 11959, 12836, 16555, 11649, 23790, 8738, 13692, 11922, 25129, 16376, 22223, 4601, 16869, 21227, 15576, 22458, 11379, 24865, 31092, 12460, 11881, 25174, 26057, 11340, 12059, 13028, 5589, 8569, 7971, 9625, 14757, 16640, 25854, 8012, 21890, 13502, 14654],\n",
        "    'Disaster': [212, 174, 105, 48, 156, 131, 172, 151, 184, 57, 205, 116, 266, 45, 239, 116, 48, 61, 74, 134, 142, 54, 98, 271, 238, 135, 103, 110, 149, 18, 79, np.nan, 132, 152, 61, 226, 2, np.nan, 118, 184, 235],\n",
        "    'ICT_Failure': [5250, 4141, 5245, 6087, 5098, 5081, 3596, 3674, 2195, 8499, 2512, 3614, 3563, 6790, 4410, 7495, 3653, 5558, 3329, 9290, 2625, 5620, 6268, 5828, 4629, 4740, 5088, 4706, 6928, 9421, 3885, 2260, 4823, 10159, 3393, 8278, 8355, 3893, 6138, 8257, 4242],\n",
        "    'Execution_DP': [4859, 9610, 11557, 10157, 10988, 5730, 3735, 12504, 7032, 7248, 11985, 9669, 9368, 14495, 6069, 10784, 8171, 8806, 12585, 9524, 9615, 11218, 6905, 5482, 4866, 2101, 14169, 4690, 7336, 7170, 8922, 10447, 10581, 6251, 7499, 8848, 7254, 12822, 9473, 10205, 9989]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Handle missing values in Disaster\n",
        "df['Disaster'] = df['Disaster'].fillna(df['Disaster'].median())\n",
        "\n",
        "# Prepare features and target\n",
        "X = df[['Internal_fraud', 'External_fraud', 'Employ_PWS', 'Clients_PBP', 'Disaster', 'ICT_Failure', 'Execution_DP']].values\n",
        "y = df['loss'].values\n",
        "\n",
        "# Scale features and target\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Correlation Analysis\n",
        "corr_matrix = df[['Internal_fraud', 'External_fraud', 'Employ_PWS', 'Clients_PBP', 'Disaster', 'ICT_Failure', 'Execution_DP', 'loss']].corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
        "plt.title('Correlation Matrix of Features and Loss')\n",
        "plt.tight_layout()\n",
        "plt.savefig('correlation_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# Build Bayesian Neural Network\n",
        "def build_bnn_model(input_dim, dropout_rate=0.3):\n",
        "    inputs = tf.keras.Input(shape=(input_dim,))\n",
        "    x = tf.keras.layers.Dense(64, activation='relu')(inputs)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "    x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "    mu = tf.keras.layers.Dense(1)(x)\n",
        "    log_sigma = tf.keras.layers.Dense(1)(x)\n",
        "    outputs = tf.keras.layers.Concatenate()([mu, log_sigma])\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    def negloglik(y_true, y_pred):\n",
        "        mu, log_sigma = y_pred[:, 0], y_pred[:, 1]\n",
        "        dist = tfp.distributions.Normal(loc=mu, scale=tf.exp(log_sigma) + 1e-6)\n",
        "        return -tf.reduce_mean(dist.log_prob(y_true))\n",
        "\n",
        "    def custom_mae(y_true, y_pred):\n",
        "        mu = y_pred[:, 0]\n",
        "        return tf.reduce_mean(tf.abs(y_true - mu))\n",
        "\n",
        "    model.compile(optimizer='adam', loss=negloglik, metrics=[custom_mae])\n",
        "    return model\n",
        "\n",
        "# Monte Carlo Dropout for uncertainty estimation\n",
        "def predict_with_uncertainty(model, X, n_iter=100):\n",
        "    means, log_sigmas = [], []\n",
        "    for _ in range(n_iter):\n",
        "        pred = model(X, training=True)\n",
        "        means.append(pred[:, 0].numpy())\n",
        "        log_sigmas.append(pred[:, 1].numpy())\n",
        "    means = np.array(means)\n",
        "    sigmas = np.exp(np.array(log_sigmas))\n",
        "    mean_pred = np.mean(means, axis=0)\n",
        "    epistemic_var = np.var(means, axis=0)\n",
        "    aleatoric_var = np.mean(sigmas**2, axis=0)\n",
        "    total_var = epistemic_var + aleatoric_var\n",
        "    return mean_pred, np.sqrt(epistemic_var), np.sqrt(total_var)\n",
        "\n",
        "# Cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# Initialize lists to prevent accumulation from previous runs\n",
        "rmse_scores = []\n",
        "coverage_scores = []\n",
        "mean_preds = []\n",
        "epistemic_stds = []\n",
        "total_stds = []\n",
        "y_tests = []\n",
        "test_years_all = []\n",
        "\n",
        "for train_idx, test_idx in kf.split(X_scaled):\n",
        "    X_train = X_scaled[train_idx]\n",
        "    X_test = X_scaled[test_idx]\n",
        "    y_train = y_scaled[train_idx]\n",
        "    y_test = y_scaled[test_idx]\n",
        "    model = build_bnn_model(input_dim=X_train.shape[1])\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=8, verbose=0)\n",
        "    mean_pred_scaled, epistemic_std_scaled, total_std_scaled = predict_with_uncertainty(model, X_test)\n",
        "    mean_pred = scaler_y.inverse_transform(mean_pred_scaled.reshape(-1, 1)).flatten()\n",
        "    epistemic_std = epistemic_std_scaled * scaler_y.scale_[0]\n",
        "    total_std = total_std_scaled * scaler_y.scale_[0]\n",
        "    y_test_orig = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "    mean_preds.extend(mean_pred)\n",
        "    epistemic_stds.extend(epistemic_std)\n",
        "    total_stds.extend(total_std)\n",
        "    y_tests.extend(y_test_orig)\n",
        "    test_years_all.extend(df.iloc[test_idx]['Date'].values)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_orig, mean_pred))\n",
        "    coverage_95 = np.mean((y_test_orig >= mean_pred - 2*total_std) & (y_test_orig <= mean_pred + 2*total_std))\n",
        "    rmse_scores.append(rmse)\n",
        "    coverage_scores.append(coverage_95)\n",
        "\n",
        "# Verify sample sizes after cross-validation\n",
        "print(\"Post-CV shapes:\")\n",
        "print(f\"len(mean_preds): {len(mean_preds)}\")\n",
        "print(f\"len(y_tests): {len(y_tests)}\")\n",
        "print(f\"len(test_years_all): {len(test_years_all)}\")\n",
        "assert len(mean_preds) == 41, f\"Expected 41 samples, got {len(mean_preds)}\"\n",
        "assert len(y_tests) == 41, f\"Expected 41 samples, got {len(y_tests)}\"\n",
        "\n",
        "# Convert to arrays and sort by year\n",
        "mean_preds = np.array(mean_preds)\n",
        "epistemic_stds = np.array(epistemic_stds)\n",
        "total_stds = np.array(total_stds)\n",
        "y_tests = np.array(y_tests)\n",
        "test_years_all = np.array(test_years_all)\n",
        "sort_idx = np.argsort(test_years_all)\n",
        "mean_preds = mean_preds[sort_idx]\n",
        "epistemic_stds = epistemic_stds[sort_idx]\n",
        "total_stds = total_stds[sort_idx]\n",
        "y_tests = y_tests[sort_idx]\n",
        "test_years_all = test_years_all[sort_idx]\n",
        "\n",
        "# Train model on full dataset for importance and PDPs\n",
        "sklearn_model = KerasRegressor(\n",
        "    model=build_bnn_model,\n",
        "    model__input_dim=X_scaled.shape[1],\n",
        "    model__dropout_rate=0.3,\n",
        "    epochs=100,\n",
        "    batch_size=8,\n",
        "    verbose=0\n",
        ")\n",
        "sklearn_model.fit(X_scaled, y_scaled)\n",
        "\n",
        "# Feature Importance\n",
        "# Ensure X_scaled and y_scaled have the same number of samples\n",
        "print(\"Shapes before permutation_importance:\")\n",
        "print(f\"X_scaled: {X_scaled.shape}\")\n",
        "print(f\"y_scaled: {y_scaled.shape}\")\n",
        "result = permutation_importance(sklearn_model, X_scaled, y_scaled, n_repeats=10, random_state=42)\n",
        "feature_names = ['Internal_fraud', 'External_fraud', 'Employ_PWS', 'Clients_PBP', 'Disaster', 'ICT_Failure', 'Execution_DP']\n",
        "importance = result.importances_mean\n",
        "\n",
        "# Create Table for Feature Effects\n",
        "feature_effects = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Permutation_Importance': importance,\n",
        "    'Correlation_with_Loss': corr_matrix['loss'][feature_names].values\n",
        "})\n",
        "feature_effects.to_csv('feature_effects_table.csv', index=False)\n",
        "\n",
        "# Print Table\n",
        "print(\"\\nFeature Effects on Loss:\")\n",
        "print(feature_effects.to_string(index=False))\n",
        "\n",
        "# Partial Dependence Plots\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, feature in enumerate(feature_names):\n",
        "    pd_results = partial_dependence(\n",
        "        estimator=sklearn_model,\n",
        "        X=X_scaled,\n",
        "        features=[i],\n",
        "        response_method='auto',\n",
        "        grid_resolution=50\n",
        "    )\n",
        "    grid_values = scaler_X.inverse_transform(\n",
        "        np.repeat(pd_results['grid_values'][0].reshape(-1, 1), X_scaled.shape[1], axis=1)\n",
        "    )[:, i]\n",
        "    pd_values = scaler_y.inverse_transform(pd_results['partial_dependence'][0].reshape(-1, 1)).flatten()\n",
        "    plt.subplot(3, 3, i+1)\n",
        "    plt.plot(grid_values, pd_values, 'b-')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Predicted Loss')\n",
        "    plt.title(f'PDP for {feature}')\n",
        "    plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('partial_dependence_plots.png')\n",
        "plt.close()\n",
        "\n",
        "# 1. Enhanced Main Plot with Confidence and Prediction Intervals\n",
        "plt.figure(figsize=(14, 8))\n",
        "indices = np.arange(len(mean_preds))\n",
        "plt.errorbar(indices, mean_preds, yerr=2*total_stds, fmt='o', label='Predicted Loss (95% Prediction Interval)', alpha=0.6)\n",
        "plt.fill_between(indices, mean_preds - 2*epistemic_stds, mean_preds + 2*epistemic_stds,\n",
        "                 color='blue', alpha=0.2, label='95% Confidence Interval (Epistemic)')\n",
        "plt.scatter(indices, y_tests, color='red', label='True Loss', s=50)\n",
        "threshold = 40000\n",
        "high_risk = mean_preds > threshold\n",
        "plt.scatter(indices[high_risk], mean_preds[high_risk], edgecolors='black', facecolors='none', s=100, label='High Risk (>40,000)')\n",
        "plt.xticks(indices[::2], test_years_all[::2], rotation=45)\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Operational Loss')\n",
        "plt.title('Risk Assessment with Confidence and Prediction Intervals (Cross-Validated)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('enhanced_risk_uncertainty_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# 2. Calibration Plot\n",
        "def calibration_plot(mean_pred, total_std, y_true):\n",
        "    coverage_probs = np.linspace(0, 1, 20)\n",
        "    observed_coverage = []\n",
        "    for p in coverage_probs:\n",
        "        z = norm.ppf(1 - (1 - p) / 2)\n",
        "        lower = mean_pred - z * total_std\n",
        "        upper = mean_pred + z * total_std\n",
        "        covered = np.mean((y_true >= lower) & (y_true <= upper))\n",
        "        observed_coverage.append(covered)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(coverage_probs, observed_coverage, marker='o', label='Observed Coverage')\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
        "    plt.xlabel('Expected Coverage Probability')\n",
        "    plt.ylabel('Observed Coverage Probability')\n",
        "    plt.title('Calibration of Uncertainty Estimates')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('calibration_plot.png')\n",
        "    plt.close()\n",
        "    return observed_coverage, coverage_probs\n",
        "\n",
        "observed_coverage, coverage_probs = calibration_plot(mean_preds, total_stds, y_tests)\n",
        "\n",
        "# 3. Feature Importance Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(feature_names, importance)\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Increase in MSE')\n",
        "plt.title('Feature Importance (Permutation)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# 4. Loss Distribution for a High-Risk Sample\n",
        "high_risk_idx = np.where(high_risk)[0][0] if np.any(high_risk) else 0\n",
        "sample_means = scaler_y.inverse_transform(np.array([\n",
        "    sklearn_model.predict(X_scaled[sort_idx[high_risk_idx]:sort_idx[high_risk_idx]+1])\n",
        "    for _ in range(100)\n",
        "]).reshape(-1, 1)).flatten()\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(sample_means, bins=30, density=True, alpha=0.7, label='Predicted Loss Distribution')\n",
        "plt.axvline(y_tests[high_risk_idx], color='red', linestyle='--', label='True Loss')\n",
        "plt.xlabel('Operational Loss')\n",
        "plt.ylabel('Density')\n",
        "plt.title(f'Loss Distribution for Year {test_years_all[high_risk_idx]}')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('loss_distribution_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# 5. Prediction Interval Coverage Over Time\n",
        "coverage_by_year = (y_tests >= mean_preds - 2*total_stds) & (y_tests <= mean_preds + 2*total_stds)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(indices, coverage_by_year, c=coverage_by_year, cmap='RdYlGn', s=100, label='Within 95% Interval')\n",
        "plt.axhline(y=0.95, color='black', linestyle='--', label='Expected 95% Coverage')\n",
        "plt.xticks(indices[::2], test_years_all[::2], rotation=45)\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Within 95% Prediction Interval')\n",
        "plt.title('Prediction Interval Coverage by Year')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('coverage_over_time_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# 6. Uncertainty Decomposition\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(indices, epistemic_stds**2, label='Epistemic Variance', alpha=0.5)\n",
        "plt.bar(indices, total_stds**2 - epistemic_stds**2, bottom=epistemic_stds**2, label='Aleatoric Variance', alpha=0.5)\n",
        "plt.xticks(indices[::2], test_years_all[::2], rotation=45)\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Variance')\n",
        "plt.title('Uncertainty Decomposition (Epistemic vs. Aleatoric)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('uncertainty_decomposition_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# Print results\n",
        "print(\"Cross-Validation Results:\")\n",
        "print(f\"Average RMSE: {np.mean(rmse_scores):,.0f}  {np.std(rmse_scores):,.0f}\")\n",
        "print(f\"Average 95% Prediction Interval Coverage: {np.mean(coverage_scores):.2%}  {np.std(coverage_scores):.2%}\")\n",
        "print(\"\\nSample Predictions with Uncertainty:\")\n",
        "for i in range(min(10, len(mean_preds))):\n",
        "    print(f\"Year {test_years_all[i]}: Predicted Loss = {mean_preds[i]:,.0f}, \"\n",
        "          f\"Epistemic Uncertainty (2) = {2*epistemic_stds[i]:,.0f}, \"\n",
        "          f\"Total Uncertainty (2) = {2*total_stds[i]:,.0f}, True Loss = {y_tests[i]:,.0f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5UVOv89TP8gM",
        "YFaTZO6uiRF6",
        "9DahLpG7FHUQ",
        "djn4cB_lyqJq",
        "uMiJtv7YzN8x",
        "31DUxSNLzlmH",
        "uzNqfhuKaMWA",
        "Oj0DrSqX0DHt",
        "kA9nEUPpZtD2",
        "MMT0o9zIV92m",
        "st2Y7NMFT2uQ"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}